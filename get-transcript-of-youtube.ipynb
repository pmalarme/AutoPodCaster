{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install ffmpeg for Whisper\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get install ffmpeg\n",
    "    ```\n",
    "\n",
    "3. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU pytubefix moviepy openai-whisper langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai tiktoken openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: How to fine-tune a model using LoRA (step by step)\n"
     ]
    }
   ],
   "source": [
    "from pytubefix import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "yt = YouTube('https://www.youtube.com/watch?v=8N9L-XK1eEU')\n",
    "\n",
    "title = yt.title\n",
    "url = yt.watch_url\n",
    "description = yt.description\n",
    "thumbnail_url = yt.thumbnail_url\n",
    "\n",
    "print(f'Title: {title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video with lowest resolution as the focus is on the audio\n",
    "\n",
    "video_file_name = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(output_path='outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the information file\n",
    "\n",
    "# Escape description and replace newlines with \\n\n",
    "escaped_description = description.replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n",
    "\n",
    "# Write a file with all the information about the video as a json\n",
    "info_file_name = video_file_name.split('.')[0] + '_info.json'\n",
    "with open(get_file(info_file_name), 'w') as f:\n",
    "    f.write(f'{{\"title\": \"{title}\", \"url\": \"{url}\", \"description\": \"{escaped_description}\", \"thumbnail_url\": \"{thumbnail_url}\"}}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file name for the audio file\n",
    "\n",
    "audioFileName = video_file_name.split('.')[0] + '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /home/pmalarme/workspace/drop-all/outputs/How to fine-tune a model using LoRA (step by step).wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Create the audio\n",
    "\n",
    "video = VideoFileClip(video_file_name)\n",
    "audio = video.audio\n",
    "audio.write_audiofile(get_file(audioFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the video file\n",
    "\n",
    "video.close()\n",
    "os.remove(get_file(video_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Today we are going to be fine tuning a large model using two separate data sets. And I'm going to show you how to use Laura, how to write code to do the fine tuning process extremely efficient. Here's what's really cool about Laura. Not only makes fine tuning way faster, but it also generates these small adapters that you can plug and play together with your model to get your model self specific tasks. Okay, so when you think about this, Apple is about to release Apple intelligence. I'm going to show you right now what they say on the black post that they posted. So you can see how smart it is what they're doing. That is exactly what I'm going to show you how to code today so you can do the same thing. So let me go to the website and show you what I'm talking about. This is the machine learning research blog from Apple. And there is a section you can read this article if you want to is called introducing Apple foundation models. So they're talking about their big models that are going to be keeping inside the phone in your pocket. In your pocket. Okay, so there is a section that's called model adaptation and I think it's great. So they say our foundation models are fine tuned for users every day activities. So basically they have a foundation model and they did fine tuning for a bunch of different things that they want to use that model for like email summarization or email replies or query handling. Like there are multiple tasks and they're doing the fine tuning process to adapt that foundation model to the process to the specific tasks and can dynamically specialize themselves on the fly for the task at hand. This sentence here it's like magic. They have foundation models. They fine tune those models and they're telling us that those models can specialize on the fly. That sounds critical. We utilize adapters. We're going to be building those adapters today. I'm going to show you how to do that and I'm going to show you more important how they work. Small neural networks modules that can be plugged into various layers of the pre-trained model. So they have a pre-trained model. That's the foundation model. That's the big model. That's the heavy model and they have these small adapters and they're telling us that they can take those adapters and plug them with the foundation model to get the model act different. To fine tune our models for a specific task. So that's basically the way it works. That's what you're doing. That is what I'm going to be showing you how to do. Now here is my code and you have access to this whole notebook. You're going to final link below in the description somewhere. Here I'm using Lining Studio which is amazing if you really want to use GPUs and obviously because I'm going to be training a large model I'm going to need GPUs. As you can see this is just Visual Studio in the cloud. This is the way it works. If you're familiar with Visual Studio well you are familiar with this. You can see that here I'm just running this and I'm going to go through this whole explanation just using the CPUs because I don't want to be spending credits on GPUs but if I wanted to switch to GPU I can just click there click on switch to GPU and hear the options that I have. I trained my models. I fine-tuned my models here using an L4. I think it took about five minutes to fine-tune both models. Maybe maybe a little bit longer and that was pretty decent and I had to pay 70 cents per hour just to get that done. Okay so not a big deal. Let me go back to the code. Let me hide this and we can go down this notebook. It's going to be pretty simple but before I get into the actual code of how this works I want to show you a few slides that I put together just so you understand how freaking awesome this is. Okay so why do we care about fine-tuning and why do we care about Laura? Why is it important? Well at the beginning the whole idea is to get a foundation model which is going to be large it's going to be heavy. We want to fine-tune that model to specialize that model. So now in the case of Apple these are some of the tasks that Apple wanted to specialize their foundation model on. So Mel replies. They want a model that's capable of producing replies to an email right summarization proofreading query handling there are many other tasks. So you get your foundation model you make your foundation model better at those tasks by through a fine-tuning process. So that's just a regular fine-tuning as we know it. Now here's the thing though what Apple is telling us is that they're not creating copies of a foundation model copies that are specialized on different tasks which is what you will get from a regular fine-tuning process. Instead they're talking about adapters which are small neural networks that they can plug in into different layers of the foundation models. So in theory what they're telling us if the their users are going to be using the same foundation model they created plus small adapters they're going to make that foundation model act differently and this is huge if true right. So in the Apple's world or in the Laura world because Apple did not invent it this they're just using this tech they're going to have these small adapters which are going to be helping the foundation model get specialized on different things. And more important than anything they are going to be loading or plugging those adapters dynamically as the user needs to perform something. So they can have the foundation model loaded in memory and depending on the task that they want to perform they can plug and play one of those adapters and that is super super beneficial in comparison with the regular model where you actually have multiple copies of a huge model. So if you're trying to do summarization you have to load this huge model in memory you're trying to now switch to proofreading you will have to load this second model in memory remember these models are really really big and the phone does not have that much memory. So having the idea of the adapters something that is like a chip that you put together with the model and the model switches now it can do something much better. That idea I think is genius. Now how does this work okay here is where we get a little bit behind the scenes and this is what I want you to look at. So I have the foundation model original weights. Now obviously a model one of these large models they have multiple layers and there are multiple parameters per layer but just to simplify the idea imagine that we have only just one big layer okay so it's just one huge matrix of parameters in the case of the first one here the first image that I have here that will be the original weights of the foundation model. So these numbers instruct that model on how to do general tasks that's what the model does the foundation model. Now I'm adding up here a new matrix which is the same size which are changes. So this matrix is that adapter that I was talking about the adapter will tell the original weights how they need to change in order to give us defined tune weights. So now imagine that I have two different tasks I'm going to have two different adapters and each adapter will have just positive or negative numbers that added up to the original matrix will give us a new matrix that we know how to do a task better. That's the idea of an adapter something that I can dynamically load do one addition and get a completely new model that acts differently but there is a problem and this Laura weight changes matrix here is the same size as the original one but that's not what we wanted to build. What we wanted to have is just a small adapter something that's really small but here this looks like a freaking copy of the whole model so what's going on? Well that's where the genius bar comes in okay so the creators of the paper of Laura they realized that we can represent a big matrix as the multiplication of another two matrices that are smaller like for example these Laura weight changes which have the same parameters that I have here you can represent these matrix by the multiplication of these two smaller matrices and you can do the math if you want to just correct me here but in these matrix I have 25 values but here I only have 10 because it's a column vector with five rows that will give me five values and this is just a regular one row vector of another five values of five plus five that will be 10 values and those 10 values if I multiply them together will give me these 25 values. Do you see what I'm going with this? So the adapter is not this which is huge the adapter will be these two matrices here so I can get in the case of a model with 25 parameter 60% in reduction because 15 of those parameters I do not need I don't need to store them I don't need to represent them in any way I only need these 10 values at wrong time when I'm ready to load that adapter I can load my foundation model load the adapter do the multiplication which is a really fast operation and after I do the multiplication I'm gonna get the weight changes and then I'm gonna get those weight changes and add them up the original weights to get the fine tuned weights and that will happen extremely fast in memory and we can get a model acting differently without having to keep different copies of that model and that is genius now I want you to see something else which is gonna give you an idea of how impressively effective this is I just put here a few examples of different model sizes so you see the savings so for 25 parameters which is just for illustration purposes the savings are 60% because we only need 10 values to represent 25 for a model with 1 million parameters the savings will be 99.80% just huge how much space we are going to save right and for 2 billion parameters which is considered today a very small model the savings will be 99.995% because we will need you know for 2 billion parameters we will need approximately 44,000 times 44,000 matrix just to represent the same thing and that will give us a total of approximately 88 or 89,000 parameters total now keep in mind a couple things a number one remember this is just for illustration purposes only because a model has multiple layers so it's not like we only have a huge matrix so we will have to do these multiple times a lora adapter will not just be two matrices it will be two matrices for each one of the layers where we want to apply lora I'm just simplifying this so you think of it as a single big matrix because it's easier to explain but the same principle supply the savings will be humongous the other thing that I want to mention is that it is very clear just by looking at this happily that the savings are huge so in terms of storing this model or these adapters we're going to have like a huge decrease in memory and disk space but that's not all of it the creators of the papers also proved also realized that we can actually instead of trying during the fine tuning process instead of trying to fine tune every single parameter individually we can focus on finding or fine tuning these two matrices here so basically we can take these two matrices and fine values that multiply give us a list of changes that added up to the original model get us closer to our objective so the fine tuning process is also getting a huge boost and now we can fine tune a huge model very very quickly because we're only changing these few parameters here so it's a double band here right we're getting we're getting our cake and we're eating our cake too and that is super super cool and final thing that I'm going to say before I go to the code lora is not even the most efficient way of doing it there is like a second version it's not a second version it's a different version that's called q lora which does lora but quantize I'm not going to talk about that one right now but just look it up if you wanted to see an even more efficient way of doing this now let's see the code because that's where things get really interesting to run this code and you have two options so number one download this notebook on your computer you're gonna have to have a GPU or this is gonna take a long time okay the model is not huge but still it's gonna take a long time if you want to the fine tuning process on your computer so load this on your on your computer all of the libraries that you need are specified right here right you're gonna need transformers accelerate evaluate data sets and parameter efficient fine tuning those are all the libraries that you need so just running this notebook will give you everything that you need or you can just go to studio just create a new account in line in studio and this is what you are going to get right and now you can use GPUs you have a free quota which is great that'll be awesome so what am I going to be fine tuning here I'm gonna be fine tuning a vision transformer basically I have a large vision transformer that I want to specialize in two different ways number one I wanted to improve it recognizing food items recipes okay so I have a data set of food items that I'm gonna be using to fine tune this large model and number two I want to get the model good at recognizing cats versus ducks not a huge deal just two coupled two good examples and of course this is not a large language model this is gonna be a vision transformer so we're gonna be doing this with images first of all I start by creating a few helpful functions that I'm gonna be using throughout my notebook they're not a big deal the first one here it's just gonna tell us what the size of a model is in megabyte so I can show you the savings we are gonna be like really really big by the way the original vision transformer that I'm gonna be using is 346 megabyte in disk okay so just keep that in mind three hundred and forty six let's call it 350 megabytes on disk so this function that I created here is gonna help us just print the size of the fine tuned versions or the Laura adapters you can see like the savings how big they are this function here is gonna show me how many parameters we're gonna need to train and the reason I use it is to show you the comparison of hey this is the original vision transformer how many parameters do we have there versus how many parameters do we actually have to fine tune when using Laura so you can see also the savings in the number of parameters this is a function to just split a data set just get a 10% of that data set set it aside for testing purposes and this is a function that's gonna create a label mapping and I'm gonna explain what that is in just a second all right so after I define all of those functions then I'm gonna be loading those two data sets I'm gonna use gonna be loading those data sets from Hockingphase the multiple data sets there that you can do this on I'm just using the boot as data set number one and the Microsoft Cats versus Docs as data set number two now loading the data sets is the same process the only thing that I'm doing here that's different is for the second data set the label column is called labels with an S plural so just because I'm gonna be using the same exact process to fine tune my model on both data sets I just renamed that labels to label just so both data sets have the same columns and I can use exactly the same process so after I load the data sets I split them so I get that a set one train that a set one test and that a set two train and that a set two test okay so nothing fancy there right so after doing that I'm gonna be creating the mappings and again the mappings are just two dictionaries let's look at the function here really quick so I have two dictionaries and one isn't mapping from label to ID so it goes from the name of the label let's say it's pizza to the ID three and the name of the label which is chicken wings to the ID which is four that's the mapping right from label to ID and the second mapping is the other way around it goes three value is pizza four values chicken wings by the way I don't know if those are the actual values but just just to illustrate what the what the idea here is we need this because when we are creating these models in order to fine tune them and in order to load them so they can actually solve classification problems the process of loading them and preparing them needs to create heads to do the classification and that uses the mapping to just do that so just keep that in mind is something that you can read a little bit more under this link so I have a link here for pre-trained config and I tell you exactly where to go if you want to understand more how those mappings are gonna be used throughout my code right so I'm gonna create those two mappings and then I'm gonna create this configuration this dictionary is gonna help me basically train or fine tune both models by just using one section for model one and one section for model two so this is just saying hey when I when I'm gonna fine tune model one I'm gonna be using that I set one for train that I set one for test etc etc the differences here are the number of epochs for the first fine tune process I'm gonna use five epochs for the dogs versus cats I'm just gonna use one epoch because that's good enough I get high enough accuracy at that point I don't need to keep fine tuning after that and the other thing that it's different here is just the path what I'm going to be storing these adapters so obviously when I finish the fine tuning process I'm gonna be saving the adapters to disk and I'm gonna be saving them both adapters to two different folders all right so after doing this I need to pre-process the images so I'm gonna be loading an image processor right from the folder or the base model the you know the vision transformer was created so now I'm gonna be defining the steps that I'm gonna be using to do the pre-processing on my side so I'm gonna be using the specification plus specific steps that I want to define to pre-process my images now keep in mind this is not optimized my goal here is not to get the best result for neither one of those two datasets I'm even using the same pre-processing steps for both the cats versus dogs and the food items okay my goal here is to keep the code to a minimum to show you the point that I want to show you which is Laura and how you can load those adapters etc so if you wanted to do this a little bit better you could you can find a better pre-processing step here just to do that so what I have here is just a pipeline that's gonna do the pre-processing that resizes the images and not is here that I'm using the size specified by the image pre-process sort of we just loaded that's gonna be to 24 I'm centering the image so I'm doing some movement there I'm turning that into a tensor and finally I'm normalizing the image using the mean and the standard deviation specified by the original pre-processing okay this is my pipeline and then I have a pre-process function which is the one that we're gonna be using at the time of pre-processing the images before sending them through the model and this is just gonna get a batch values and it's just gonna call the pre-process pipeline converting the each image to RGB and you know for every image in the batch just convert it to RGB take it through the pipeline and add it to a batch with pixel values that's gonna be the key and that just very simple and finally what I do is I go through everyone both models that are in my config remember the config has two keys one for each model that I want to fine tune and I'm gonna grab the train data and I'm gonna specify that the transformation process for the train data is just gonna be the function that I just created right this function here and I'll do the same for the test data and because this is within a loop that's gonna go for both models it's gonna do these two steps twice so one for model one that I said number one what one for model two that I said number two okay so that prepares everything that gets my data ready for both data sets to do the fine tuning process let's do the fine tuning process now all right so how do we do that well first here I have I think I have a few functions yes so I have a few functions here that are going to I'm gonna be using the fine tuning so the first function is the collake function and what this does is basically gets the data gets a batch of data ready to be processed by my model so as you can see prepare a batch of example from a list of elements of the train or test data sets so I'm gonna be passing that list of examples and this just does the transformation using torsh to just stack the values in a way that my model understands how to process them not a big deal this is sort of like a plumbing function that I need just to do the processing the compute matrix this is the one that's gonna compute the accuracy of a batch of predictions so I'm gonna be using accuracy here to understand whether my fine tuning process is working or not and this just compute matrix is the function that's gonna do that so I pass this parameter here and from there you can see how I get the predictions and how I compute the accuracy all right the get based model this function that I created here it's basically loading the model from the model checkpoint the original vision transformer google vision transformer model so this is loading that model in memory not is how I pass the mappings that I created so when I load that model because this is gonna be a model for image classification I need to prepare that model with a head to do classification and to do that I'm gonna need to specify the mappings here all right so not complicated to understand loads the original model this is the function that I'm gonna use to fine tune the model first and later on when we're ready to do inference we're gonna be using this function as well to load that model once in memory okay finally this function here builds the Laura model so this is what we are going to be actually fine tuning okay so this is how it works first of all I load the base model I'm gonna load the big model in memory then I'm just gonna print the number of parameters so this is just so you see the lock how many parameters has the base model how many parameters do we have to fine tune from the base model then I'm just gonna create the configuration now remember in the images that I show you that I only show you rank one image matrices so if we go up you'll see here when we decompose this big matrix I show you two vectors that multiply together we'll give you the big matrix these two vectors are using rank one there's only one column vector here and only one row here of course the lower the rank the bigger the trade-off because the harder it's gonna be to be accurate with the fine tuning process so if you wanted to be more accurate you could increase the rank so for example rank two we give us a matrix that will have five rows and two columns and a matrix that will have two rows and five columns and that will be a total of 20 values so you will have 20 values to represent 25 so you can be way more precise with a higher rank now in the paper what they talk about is that after eight rank eight I think or maybe between rank eight and 256 they found out that it really doesn't matter that you can get very good approximations around that range so you don't need a huge rank just to represent to get a very good fine tuning process for this example here I'm gonna be using rank 16 so if we go back to the function you can see this R that's the rank some using rank 16 to represent these adapters okay I could go lower than that I could go to eight it will get similar results all right so here a bunch of other habit parameters the alpha if you want to know how the alpha works you can take a look at the paper it's basically when you add up the adapter to the original weight how much is that adapter matters like you multiply it basically you you are gonna multiply that and depending on how big the alpha is or how small it is is how much that is going to change the original weights you can read more in the paper what are the modules that we're gonna be modifying the dropout the bias and the modules that we're gonna be saving is gonna be the classifier so this specifies how I want to do Laura and now I can use this function which is get parameter efficient fine tuning process I'm gonna pass the original model and the configuration of Laura that I would just define and this is going to give us the actual model that we are going to be fine tuning and in this case right after this line I'm printing how many parameters has this model and you're gonna see the results as soon as I call those functions but they're gonna be way fewer parameters okay way fewer parameters because we are using rank 16 for the adapters so those are gonna be smaller matrices all right so the next line here just specify a batch size and these are all of the parameters of the the training process in this case is just gonna be fine tuning and you can go through all of those parameters they're really not that big of a deal let me see if there is something here worth mentioning the metric that we're gonna be using is accuracy learning rate is right here the batch size is 128 we're gonna be using flop point 16 to do this this training you need a GPU to do that by the way so not a big deal that defines the arguments here is where the fine tuning process happens not is that I'm gonna go for both models so I'm gonna do fine tuning right one after the other so I'm gonna get first the first model and I'm going to say hey set the number of epochs that I want for this model remember for the first that I set five epochs for the second one it's gonna be one then I'm gonna configure a trainer and the trainer the first parameter of the trainer is wish model are you gonna be fine tuning and this is what I'm passing the result of the build Laura model function which is gonna give me that Laura model to fine tune I passed the arguments I specify what is the trained data that I'm gonna use to for fine tuning what is the evaluation data that I'm gonna be using to evaluate the fine tuning process what is the tokenizer that I'm gonna use in this case is gonna be the image processor that I loaded how am I gonna compute the metrics which in this case it's just gonna be the function that I created before with compute metrics and how I'm gonna be preparing the data just to take it through the model and that's the collate function that I created before so again it's just putting together the Lego pieces that I built before just to get them at this point and then I'm gonna just train my model this is gonna do the fine tuning process when I'm done I'm going to evaluate the model this is just gonna be running a evaluation here and finally I'm just gonna print the accuracy after I finish training and after I finish evaluating the model I'm going to be saving that model I'm printing the size on disk of that specific saved model remember this saving here it's just saving the Laura adapter not the huge model but just the adapter so here are the results which of course I'm not gonna run here on camera because they will take forever here are the results that we are going to get I want you to read this specifically okay so the base model trainable parameters okay when we loaded the original model division transformer it has 85 million parameters that's how many parameters are trainable 85 million out of 85 total that's 100% of parameters are trainable so doing just regular fine tuning we will have to train or fine tune 85 million parameters instead we're gonna be doing Laura so Laura trainable parameters are only 667000 which is 0.77% of the original model that's nothing that's why we can fine tune these models so freaking fast okay awesome so that will be the first one it goes through all of the epochs here you can see the accuracy going up to 94% you can see the evaluation accuracy is 0.94 6 and the size of the model remember we have 346 346 megabytes the original model the size of this adapter in disk is only 2.7 megabytes only 2.7 megabytes on disk which is nothing compared to 346 so I'm doing just the same thing for the second one for the second dataset you get all other results so this is the interesting part because now I have these smaller adapters and I have the big model I don't have to load the big model multiple times when I have just I just need to change plug and play the adapter on that model so this is what I'm doing I have a function here that's called build inference model okay and I'm gonna pass the mappings that I need to configure the heads the classification heads and the adapter path or the path where the adapter is so I can just load the model using the get get based model that just goes loads the original model and then I can just build a parameter efficient fine tuning model so using this class using the front pre-trained I can pass my model and the adapter and this is going to return and I'll ready modify it or fine tune model that specializes in solving one specific tasks and that is just amazing I did not have to modify the original big model at all the original big model is the original big model and stays unchanged I only had to create the adapter and then plug and play that adapter at the time of using it so this is that function is gonna create an inference model and there is a predict function here I'm gonna pass an image to that function the model that I'm gonna use and the image processor because I obviously have to pre-process that image before I send it to the model so internally what I do is just I just process that image and then I go and compute the outputs compute get the luggage and just find the most important item on those log it's this is a classification head and just using the mapping I can get what the name is so hey given this ID what is the name of that class so I just the ID2 label mapping so very simple function here let's go to what's gonna happen now I'm gonna create I'm gonna add some parameters to the configuration so what is the inference model that each model is going to use and what is the image processor that each model should use and here by the way they're using both of them are using the same image processor I just kept this just in case I wanted to just make modifications before but right now both are using the same image processor both datasets this is very simple here I have a few samples so this example here is I guess we're gonna have to open these chicken wings this is these are chicken wings let's see the second one this is just a little cut the third picture is just a dog and the final picture is a pizza okay so I have four samples and notice that I'm specifying this is an image and this is the model that I want to use to classify this image so for the chicken wings and the pizza I'm gonna be using model one which is the one we find tune on food items and for the cat and the dog I'm gonna be using model two which is the one we find tune in the cat versus dogs okay so these are my samples and this is just a loop I'm gonna go through all of the samples and I'm gonna be doing the following I'm gonna be opening the image URL okay gonna get that image URL I'm gonna grab the inference model and the image processor from the specific model specifying these samples samples as model one so I'm gonna I'm gonna say okay just give me the inference model that specialized on this task and give me the image processor and then I'm just gonna be just computing the prediction so just give me a prediction given this image this inference model and this image processor and print that prediction and here is what I get at the chicken wings cat dog and pizza and that is amazing because of course anyone can build a classification model but I want you to think about what's happening here I can have my big model one big model do the fine tuning process for multiple smaller specialized models and save those small adapters and load the adapters dynamically in memory as I need them instead of having multiple copies of humongous models sitting there that is really really powerful now I want you to think about what's about to happen with this tech as phones get more powerful as hardware keeps going up or improving to catch up to where we are right now we're gonna be able to do even more personalization with techniques like Laura and Kulora or even newer techniques when we come up with them where models are not just going to be specialized on specific tasks but specialize for one specific user so I can have a model that's fine tune on my jada not somebody else's data a model that's fine tune in a way I write I like my emails I do my image changes or whatever AI helps me with that is what we're moving with this we're gonna have multiple a bacillion number of adapters small adapters that we can plug and play with a big model to help us use that model to perform multiple multiple tasks again you can find the source code here in the description below hopefully this explanation made sense leave me comments leave me a comment if there is a question that you have there is something else that I can explain or what you want the next video to be and I'll be right there with you I'll see you in the next one bye bye\n"
     ]
    }
   ],
   "source": [
    "# Create the transcript using Whisper\n",
    "\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "result = model.transcribe(get_file(audioFileName))\n",
    "\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the audio file\n",
    "\n",
    "os.remove(get_file(audioFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcript\n",
    "\n",
    "transcriptFileName = video_file_name.split('.')[0] + '.txt'\n",
    "\n",
    "with open(get_file(transcriptFileName), \"w\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt to improve each transcript chunck using the title and the description of the video (technologies, people names, etc.)\n",
    "\n",
    "prompt_template = \"\"\"Given title and description of a video, can you check its transcript and correct it. Give back only the corrected transcript.\n",
    "\n",
    "Title: {title}\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gpt-4o model client\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the transcript in semantic chunks of maximum 500 tokens containing full sentences\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "encoding_name = 'gpt-4o'\n",
    "\n",
    "# Split the text in chunks of maximum 500 tokens with '.' as separator without using langchain\n",
    "sentences = result['text'].split('.')\n",
    "chunks = []\n",
    "chunk = ''\n",
    "chunk_number = 1\n",
    "for sentence in sentences:\n",
    "    if num_tokens_from_string(chunk + sentence, encoding_name) > 500:\n",
    "        prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "        corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        ).choices[0].message.content\n",
    "        chunks.append(corrected_chunk)\n",
    "        chunk = sentence + '. '\n",
    "        chunk_number += 1\n",
    "    else:\n",
    "        chunk += sentence + '. '\n",
    "        \n",
    "# Write the last chunk\n",
    "prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ").choices[0].message.content\n",
    "chunks.append(corrected_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full corrected transcript and add white-lines between the chunks but not for the last chunk\n",
    "\n",
    "full_corrected_transcript = ''\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    full_corrected_transcript += chunk\n",
    "    if i < len(chunks) - 1:\n",
    "        full_corrected_transcript += '\\n\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the full corrected transcript\n",
    "\n",
    "full_corrected_transcript_file_name = video_file_name.split('.')[0] + '_corrected.txt'\n",
    "\n",
    "with open(get_file(full_corrected_transcript_file_name), \"w\") as f:\n",
    "    f.write(full_corrected_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create langchain document\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "document = Document(\n",
    "  page_content=full_corrected_transcript,\n",
    "  metadata={\n",
    "    \"title\": title,\n",
    "    \"source\": url,\n",
    "    \"description\": description,\n",
    "    \"thumbnail_url\": thumbnail_url,\n",
    "    \"page\": 0,\n",
    "    \"type\": \"video\"\n",
    "  }\n",
    ")\n",
    "\n",
    "documents = [document]  # List of documents to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast based on the video \" + title + \".\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = video_file_name.split('.')[0] + '_podcast_outline.txt'\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = video_file_name.split('.')[0] + '_podcast_script.json'\n",
    "\n",
    "with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-GuyNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-JennyNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = video_file_name.split('.')[0] + '_podcast.wav'\n",
    "stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
