{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install ffmpeg for Whisper\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get install ffmpeg\n",
    "    ```\n",
    "\n",
    "3. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU pytubefix moviepy openai-whisper langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai tiktoken openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Azure AI Studio vs Copilot Studio\n"
     ]
    }
   ],
   "source": [
    "from pytubefix import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "yt = YouTube('https://www.youtube.com/watch?v=8MMoBiIj9hI')\n",
    "\n",
    "title = yt.title\n",
    "url = yt.watch_url\n",
    "description = yt.description\n",
    "thumbnail_url = yt.thumbnail_url\n",
    "\n",
    "print(f'Title: {title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the information file\n",
    "\n",
    "# Escape description and replace newlines with \\n\n",
    "escaped_description = description.replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n",
    "\n",
    "# Write a file with all the information about the video as a json\n",
    "info_file_name = fileName.split('.')[0] + '_info.json'\n",
    "with open(info_file_name, 'w') as f:\n",
    "    f.write(f'{{\"title\": \"{title}\", \"url\": \"{url}\", \"description\": \"{escaped_description}\", \"thumbnail_url\": \"{thumbnail_url}\"}}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video with lowest resolution as the focus is on the audio\n",
    "\n",
    "fileName = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file name for the audio file\n",
    "\n",
    "audioFileName = fileName.split('.')[0] + '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /home/pmalarme/workspace/drop-all/Azure AI Studio vs Copilot Studio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Create the audio\n",
    "\n",
    "video = VideoFileClip(fileName)\n",
    "audio = video.audio\n",
    "audio.write_audiofile(audioFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the video file\n",
    "\n",
    "video.close()\n",
    "os.remove(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you're wondering about the difference between Azure AI Studio and Copilot Studio, or in fact, Azure Open AI Studio and the bot framework and all of these pieces that allow us to create Copilot, I'm going to take you through an explanation in a demo here to help you get this sorted out. So Azure AI Studio and Copilot Studio are actually two quite different things, and it's not necessarily one or the other they do work together. So to start with, what I'm going to do is give you a bit of a demo walkthrough of Azure AI Studio, and then we're going to spend most of the time having a look at the Copilot Studio experience so that you can get that real sense and understanding of what these tools do and how you can choose which one to use and when. So help you understand this, I'm going to frame this up using three C words, cost complexity, and another phrase called conversational orchestration. Now I reckon as we go through this video, I'm going to come up with more C words, don't worry, we're not going anywhere near anything that we shouldn't put on YouTube. Let me know how many you find us I go along here. Copilot is also going to be another one of them. Let's start here with Azure AI Studio. Now this is a place that you start if you want to do something custom, you want to customize AI solution, you want a lot of control over what you're doing. The concept here is that you are starting with your AI model and having a lot of control over what you can do with it, but that's the starting point and you can do so much here. This is not a full tutorial on Azure AI Studio, but this should be enough to give you a sense of what it is. So this is my opening screen. You've got all of these things that you can do here. If I click on explore, you'll see the first thing that happens is that you are given the option to find the right model to build your custom AI solution. So this is the starting point. These are all sorts of models in here at the time of recording here. We've got 1,624 of them. Let's go in and click on the filters and you'll see that we've got things here that are models that are coming from Microsoft as well as meta as your open AI, hugging face, all of these different things. You've actually got the ability to come in here and choose to benchmark your models. So let's say I want to and I'm just going to pick it random because honestly for me, this is not my world. I don't know all of these things, but let's sort of pick out a couple of things here just to sort of put them into this comparison chart and you'll see you're getting things that are telling you about the model accuracy, the model coherence. There are a lot of metrics in here and numbers. Now if you know what these things are, this is your world. If you don't, this is probably not going to be for you and when we get to co-pilot studio, you'll see how that actually allows you to start from a different place. We're all this is taking care of for you, but if you want to be able to say I want to use the GPT-3 5 turbo model or this one or the Lama model or whatever it is or I'm looking for models that will generate images or use vision, things like that, then you can start here and have full control over all of those things. You are also responsible here if you're doing it this way for building, deploying, everything. So you have that greater control, you have that ability to do those customized things, but it's also going to mean that the time to deployment is much greater, the cost of building it is going to be much greater, plus you are actually paying for whatever services you choose here. So that cost is a really important factor. So you'll end up with something like this where you have a project, you've got all of these different tools in here where you can fine tune the model work with the playground as I said, not so much my area, but lots of things in here that allow you to tweak and play with all of this stuff. Importantly, you've also got control over the search, the index, the data sources. So this is something where we're using the broader Azure tools here. I've got the search in here so we could come in here and say I want to work with my data and you've actually got all of these different kinds of things so you could be working with your data insider, SQL database, file storage, Cosmos blob storage. This could be at scale and then you've also got control here of doing things with semantic ranking, vector indexing, all of these things around the search. Now I said I'm not going to go deep diving into all of that, but essentially as your open AI is sorry, I'll come back to what Azure Open AI is in a moment. Azure AI is a place where you can work with all of those models in a very custom way and you're building a custom AI solution with control over searching, indexing all of those kinds of things, but the complexity is quite high. Azure Open AI Studio, which I accidentally mentioned before, very similar concept, but this is just the open AI models. So this is where you're working with those models that are used underneath chat GPT, whereas the Azure AI Studio is actually giving you, if we go back to our explore here, models across a whole lot of other providers. So those are the tools that are available to you in there. Now the difference between these things and working with Copilot Studio, just as a starting point, if I go back to my home here, look at all these things, we're starting to talk about custom AI assistance, Azure AI on your data, multi-modality SDK code is another factor in here, but it's not as simple as just Procode versus Lowcode, but it's another C word for you. This is the starting point for Copilot Studio, a completely different experience. Now this is a SaaS product. So the difference here is that if you're working with Azure AI Studio, you are building, deploying, maintaining, doing all of those things yourself from start and all of the maintenance as you go. This is basically ready to go for you, providing a bunch of stuff out of the box. I don't have to, in fact, I can't choose my model as a starting point here, but importantly, this is a tool that is allowing you to do some other things. So you can build your own custom Copilot. That's the first option there, which is basically building out an experience that can contain both Generative AI and conversational orchestration. We'll come back to that in a moment. You can also extend the Microsoft Copilates that are already provided. So Microsoft 365 Copilot is the main one here. That's the one that sits inside Teams and Word and Outlook and all of those things. So if you want to take that experience and extend it to plug into your data sources, but run the whole thing through that Microsoft 365 chat experience, you can do that but to be clear, this product comes with its own standalone license. It's US $200 per tenant per month, depending on how much you're using. You can add capacity in there. You don't have to have Copilot from Microsoft 365 to use this, but if you do have this, this is the tool that you can use to extend that. I'm going to focus on the Creator Custom Copilot here for the rest of this video, though, because I don't want to confuse those things with the first party Copilot, even though that's the extensibility experience in here. So what this allows you to do is to create your own custom Copilot. Like I said, this is an out-of-the-box solution. Think of it as a nice analogy of if you if you're old enough to remember having to hire videos or hire DVDs, you have to maintain the machine that you use and you go out and get them and you've got all this hardware. This is like Netflix. You've signed up and it's just there and you just consume what you want. So this ability to, you don't have to maintain, you don't have to deploy anything. It's ready to go. So what happens here when you come into Creator Custom Copilot is you have options to do two quite different things more than two, but the two that I'm going to focus on here. So if we go to say Creator Custom Copilot, the first thing that happens is it says, do you want to give it some knowledge with Generative AI by entering your website? You don't have to just use a website here. You can use this to put in documents from SharePoint. You can upload documents. Of course, I've got one I've prepared earlier. My favorite one here is the Better Healthbot, which is actually connected to a website that is called the Better Health Channel. This is a public service website in the part of the world where I work that helps people find health information. It's very information rich and a really good example that I like to use here. So I've basically pointed my copilot at that experience. So let's talk about the Generative AI experience first and then the conversational orchestration experience, which is a really important part of what you get if you work with Copilot Studio. That's very different from Azure Open AI. So as your Open AI, I've chosen my model. I've worked with all these complex data sources picked up my indexing all of those kinds of things. This one all I've done is paste that you are willing and that's it. So I can just come in here and say what should I do if I need to call an ambulance? And what that's going to do is just take my question, refer to that data source and it's using the services that are built in here to generate an answer for me, including a link back to that website. Now I didn't do anything here other than point to that URL. What this is doing under the hood, it's using a GPT 3.5 turbo model at the time of recording, which is April 2024. That's actually hosted on the Azure Open AI service underneath. But I didn't do anything about that. I don't need to know that. I'm telling you that because it's in the documentation. But this is this idea of a SaaS product. I can just come in and that's all set up for me. So the difference here is that there's no complexity. The cost is much more controlled, but I don't have the amount of control and customization that I had when I was looking at the Azure Open AI service or the Azure AI service. The idea here though is that the time to value the time to being able to use it is super quick. I can just plug in my data source and it's ready to go. So you really have to think about what kind of solution you're looking for. Do you really need something custom? Really. These large language models and the large language model you've got access to here is highly sophisticated. If you're going to go in and start customizing large language models or building your own models, that is something that needs a team of data scientists. It needs a lot of expertise. And awesome. If you've got that in your use case, genuinely needs it and you'll pay the costs associated with it. So you need to weigh up that cost complexity customization. But do consider this. This is actually something that with some of the tweaks and I've got a whole video here on how to get the most out of this general events as capability. You can actually go quite far with this. So just really weigh up the costs of those things and what you actually need. The other really important part, the other the third C. How many of you might have to so far? And by the way, don't forget to give this video a like. It helps get it out to other people and comment how many C's you've found in here. So the other thing here is this conversational orchestration. What we mean by that is that if your whole solution is just Generative Answers, which is what I've done here, I can just plug that in or plug in my SharePoint data sources or whatever it is and off it goes. And that's it. But that's not always the case. So we've got an example here, for instance, where we've got what should I do if I need to call an ambulance? So we might want for this particular question, instead of just directing people to that website, we might want to give them some more information. So one of the things that happens in Australia here is that there are other services other than an ambulance. Some people will call an ambulance for things that aren't emergencies and block up the system. So what we can do here is direct them in a controlled way. This is this idea of conversational orchestration. I want to actually control the experience. So that for certain topics, the conversation is forced down a predetermined path with things like variables and answers and specific questions, or I could plug into other systems. So I want control over some of those things. And then this generative answers is my fallback. And this is quite common. If you think about your use case, if you're just starting with a model and that's all you want, your whole solution is just generative AI with Azure Open AI, that's awesome. But if you want this control over the topic, then this is something you can do here. The Azure Bot Framework, which has now got a different name that I'm just going to check on the side here because they do keep changing things. So this is also called the Azure AI bot services. Now you can do a similar thing there. That is a high code solution. Now you are actually starting to customize things. You'll have more control over the interface of what's going on. Not going to demo that one here, but that is a sort of a high code way of doing it. And these things do work together as well. So let's have a look at this concept of a topic. I'm going to come in here and I'm going to create a new topic. Now this is where it gets a little bit a little bit meta. So we're going to go, I'm going to create a topic from, I'm going to use co-pilot to build me a topic for my co-pilot. Are you following? This is essentially the ability to say, let's call this one, calling an ambulance or whatever it is. I can say create a topic too. So if you saw the old palvich agent, so you've worked with creating bots in the past, you have to give it sort of examples of what are the prompts, what are the things that the user is going to type in that will trigger this conversational path. You don't even have to come up with the phrases yourself now. This is actually really clever. So I'm just going to copy paste something I've done before, rather than watching me type, help a user to understand when they should call an ambulance and when they should go to a doctor instead. So this is now going to create a topic for me. And I can edit it. So we'll, let me go, that's come up faster than I can explain it. We're going to bring this along here. So it's come up with the trigger phrases here, call an ambulance, emergency medical assistance, need immediate help. So these are the kinds of things that the person might type. And this is actually going to then redirect when people do that into this topic. It's actually come up with a message generated for me. Now I can edit all of these things. I will show you the editing in a moment. So here we go, call an ambulance if, visit a doctor if this is pretty good. So what I can do now, let's just go over here and grab this one again. What should I do if I need to call an ambulance? And you'll see what happens now is that it's gone into this topic. So we can see the little ticks on the side. I'm working this by the way, just all in the test studio. In the end, this copilot actually gets published somewhere externally on a website or if it's an eternal use case into teams as a heap of channels and things, but I'm just showing you the mechanics behind the system here. It does become not just in the test canvas, but you'll see it's actually gone into this ambulance topic. And instead of the first time around, it's used generate advances as that fall back to find the answer from the data source I gave it. Now it's coming in and giving me the controlled path. Let's do something a little bit more sophisticated here. We're going to create another topic from a description. Again, we'll call this one ambulance and I'm going to give it a little bit more. So the first sentence is the same as what we had before, but now I'm saying the bot will be used in Australia where the emergency number is triple zero. Ask if the emergency is life threatening. If the emergency is not life threatening, they should go to a priority primary case and to try saying that 10 times fast. Now hopefully someone genuinely using an emergency is making a phone call, but anyway, we're helping them out. So you'll see what happens here. We'll just close this down is that it's come up with something different. So we've got those trigger phrases again. You can edit the trigger phrases. So you'll see here the dial 911 for an ambulance is actually something that isn't quite right for my context. So I can actually go in and change those things as well. And then what we've got is the it's actually created a whole topic here that says is the emergency life threatening. It's looking for a Boolean and a Boolean answer of yes or no. And then if it is, we're going to go to this message that says please call triple zero immediately. And if it's not, then it's going to do that. And then we could actually add in a link to those things. You could use this to escalate to an agent. You could use this to post to another system or get information from another system. So you can do a whole lot here with the topics. And this is this idea of conversational orchestration. So co-pilot studio actually, let's demonstrate this before we go any further. So the same question that I had before, what should I do if I need? An ambulance. And we'll watch it go through here. So it's coming up with this question. Is the emergency life threatening? No. And now it's going to go down that path. So co-pilot studio here is actually doing two different things. It's doing that generative AI stuff plug into your data and just answer those questions. But it's also able to do that conversational orchestration thing so that you have control over the things that you want to have control over. How many C's did you find in the video? By the way, I think there's at least six in there. So the concept here, Azure AI Studio, you're going to use that where you want control over your model, customization over your model, if you're primarily about building out a complex AI solution. Whereas if you're in a world of do you want to be able to create this custom co-pilot, you want to be able to not have to deal with all of the maintenance and all of those things and you just ready to go and you want to do this conversational orchestration. That's all good as well. The thing where we do have the crossover of being able to use both is that you can actually use this generative answers to pull in a model that you've made in Azure Open AI. So if you do need both of these things, you want this lovely out of the box conversational orchestration experience and the complexity of a model, you can do that. Check out my video here for how you can do that and thank you for watching.\n"
     ]
    }
   ],
   "source": [
    "# Create the transcript using Whisper\n",
    "\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "result = model.transcribe(audioFileName)\n",
    "\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the audio file\n",
    "\n",
    "os.remove(audioFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcript\n",
    "\n",
    "transcriptFileName = fileName.split('.')[0] + '.txt'\n",
    "with open(transcriptFileName, \"w\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt to improve each transcript chunck using the title and the description of the video (technologies, people names, etc.)\n",
    "\n",
    "prompt_template = \"\"\"Given title and description of a video, can you check its transcript and correct it. Give back only the corrected transcript.\n",
    "\n",
    "Title: {title}\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gpt-4o model client\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the transcript in semantic chunks of maximum 500 tokens containing full sentences\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "encoding_name = 'gpt-4o'\n",
    "\n",
    "# Split the text in chunks of maximum 500 tokens with '.' as separator without using langchain\n",
    "sentences = result['text'].split('.')\n",
    "chunks = []\n",
    "chunk = ''\n",
    "chunk_number = 1\n",
    "for sentence in sentences:\n",
    "    if num_tokens_from_string(chunk + sentence, encoding_name) > 500:\n",
    "        prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "        corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        ).choices[0].message.content\n",
    "        chunks.append(corrected_chunk)\n",
    "        chunk = sentence + '. '\n",
    "        chunk_number += 1\n",
    "    else:\n",
    "        chunk += sentence + '. '\n",
    "        \n",
    "# Write the last chunk\n",
    "prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ").choices[0].message.content\n",
    "chunks.append(corrected_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full corrected transcript and add white-lines between the chunks but not for the last chunk\n",
    "\n",
    "full_corrected_transcript = ''\n",
    "for i, chunk in enumerate(chunks):\n",
    "    full_corrected_transcript += chunk\n",
    "    if i < len(chunks) - 1:\n",
    "        full_corrected_transcript += '\\n\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the full corrected transcript\n",
    "\n",
    "full_corrected_transcript_file_name = fileName.split('.')[0] + '_corrected.txt'\n",
    "with open(full_corrected_transcript_file_name, \"w\") as f:\n",
    "    f.write(full_corrected_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create langchain document\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "document = Document(\n",
    "  page_content=full_corrected_transcript,\n",
    "  metadata={\n",
    "    \"title\": title,\n",
    "    \"source\": url,\n",
    "    \"description\": description,\n",
    "    \"thumbnail_url\": thumbnail_url\n",
    "  }\n",
    ")\n",
    "\n",
    "documents = [document]  # List of documents to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Podcast Outline: Azure AI Studio vs Copilot Studio\n",
      "\n",
      "#### 1. **Introduction**\n",
      "   - Welcome and brief introduction of hosts\n",
      "   - Overview of the episode's topic: Azure AI Studio vs Copilot Studio\n",
      "   - Purpose and what listeners can expect to learn\n",
      "\n",
      "#### 2. **Understanding Azure AI Studio**\n",
      "   - Definition and primary purpose\n",
      "   - Key features and functionalities\n",
      "   - Use cases and scenarios where Azure AI Studio excels\n",
      "   - Importance of customization and control\n",
      "   - Brief demo walkthrough highlights\n",
      "\n",
      "#### 3. **Exploring Copilot Studio**\n",
      "   - Definition and primary purpose\n",
      "   - Key features and functionalities\n",
      "   - Use cases and scenarios where Copilot Studio excels\n",
      "   - Difference from Azure AI Studio: SaaS product, ease of use, and out-of-the-box solutions\n",
      "   - Brief demo walkthrough highlights\n",
      "\n",
      "#### 4. **Comparative Analysis**\n",
      "   - Direct comparison of Azure AI Studio and Copilot Studio\n",
      "   - Discussing the three C's: Cost, Complexity, and Conversational Orchestration\n",
      "   - Additional considerations: Procode vs. Lowcode, Maintenance, and Deployment\n",
      "\n",
      "#### 5. **Choosing the Right Tool for Your Needs**\n",
      "   - How to decide between Azure AI Studio and Copilot Studio\n",
      "   - Factors to consider: Project requirements, team expertise, budget, and long-term goals\n",
      "   - Examples and case studies\n",
      "\n",
      "#### 6. **Q&A Session**\n",
      "   - Address common questions and concerns from the audience\n",
      "   - Clarify any misunderstandings and provide additional insights\n",
      "\n",
      "#### 7. **Conclusion**\n",
      "   - Recap of key points discussed\n",
      "   - Final thoughts on the importance of choosing the right tool\n",
      "   - Call to action: Encouraging listeners to explore both tools and determine which fits their needs\n",
      "\n",
      "#### 8. **Outro**\n",
      "   - Thanking the audience for listening\n",
      "   - Information on where to find more resources and future episodes\n",
      "   - Social media plugs and contact information for feedback and questions\n",
      "\n",
      "#### 9. **Bonus Segment (Optional)**\n",
      "   - Fun activity: Count the number of 'C' words mentioned in the episode\n",
      "   - Announcing winners or notable mentions in the next episode\n"
     ]
    }
   ],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast based on the video \" + title + \".\"})\n",
    "podcast_outline = podcast_outline_response['answer']\n",
    "print(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = fileName.split('.')[0] + '_podcast_script.json'\n",
    "\n",
    "with open(podcast_script_file_name, \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-GuyNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-JennyNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = fileName.split('.')[0] + '_podcast.wav'\n",
    "stream.save_to_wav_file(podcast_filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
