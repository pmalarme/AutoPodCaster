{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install ffmpeg for Whisper\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get install ffmpeg\n",
    "    ```\n",
    "\n",
    "3. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU pytubefix moviepy openai-whisper langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai tiktoken openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Future of Knowledge Assistants Jerry Liu\n"
     ]
    }
   ],
   "source": [
    "from pytubefix import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os\n",
    "\n",
    "#yt = YouTube('https://www.youtube.com/watch?v=8N9L-XK1eEU')\n",
    "yt = YouTube('https://www.youtube.com/watch?v=zeAyuLc_f3Q')\n",
    "\n",
    "\n",
    "title = yt.title\n",
    "url = yt.watch_url\n",
    "description = yt.description\n",
    "thumbnail_url = yt.thumbnail_url\n",
    "\n",
    "print(f'Title: {title}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video with lowest resolution as the focus is on the audio\n",
    "\n",
    "video_file_name = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(output_path='outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the information file\n",
    "\n",
    "# Escape description and replace newlines with \\n\n",
    "escaped_description = description.replace('\\n', '\\\\n').replace('\"', '\\\\\"')\n",
    "\n",
    "# Write a file with all the information about the video as a json\n",
    "info_file_name = video_file_name.split('.')[0] + '_info.json'\n",
    "with open(get_file(info_file_name), 'w') as f:\n",
    "    f.write(f'{{\"title\": \"{title}\", \"url\": \"{url}\", \"description\": \"{escaped_description}\", \"thumbnail_url\": \"{thumbnail_url}\"}}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file name for the audio file\n",
    "\n",
    "audioFileName = video_file_name.split('.')[0] + '.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /app/src/notebooks/outputs/The Future of Knowledge Assistants Jerry Liu.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Create the audio\n",
    "\n",
    "video = VideoFileClip(video_file_name)\n",
    "audio = video.audio\n",
    "audio.write_audiofile(get_file(audioFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the video file\n",
    "\n",
    "video.close()\n",
    "os.remove(get_file(video_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/src/notebooks/outputs/The Future of Knowledge Assistants Jerry Liu.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file(audioFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:14<00:00, 9.96MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey everybody, I'm Jerry, co-founder and CEO of Lama Index, and I'm excited to be here today to talk about the future of knowledge assistance. So let's get started. First, you know, everybody's building stuff with LMZ's days. Some of the most common use cases we're seeing throughout the enterprise include the following. It includes like document processing, tagging, and extraction. It includes knowledge search and question answering. If you followed our Twitter for the past year or so, basically, you know, we've talked about RAG probably 75% of the time. And also, you start generalizing that question answering interface into an overall conversational agent that can not only, you know, do one-shot querying search, but actually store your conversation history over time. And of course, this year, a lot of people are excited about building agent workflows that can not only synthesize information, but actually perform actions and interact with a lot of services to basically get you back the thing that you need. So let's talk about specifically this idea of building a knowledge assistant, which, you know, we've been very interested in since the very beginning of the company. The goal is to basically build an interface that can take in any task as input and get back some sort of output. So the input forms could be, you know, a simple question. It could be a complex question. It could be a vague research task. And the output forms could be a short answer. It could be a research report or it could be a structured output. RAG was just the beginning. Last year, I said that RAG was basically just a hack. And there's a lot of things that you can do on top of RAG to basically make it more advanced and sophisticated. If you build a knowledge assistant with a very basic RAG pipeline, you run into the following issues. First is a naive data processing pipeline. You know, you put it through some basic parser, do some sentence splitting, chunking, do top-care retrieval. And then you realize, you know, even if it took you 10 minutes to set up, that it's not suitable for production. It also just doesn't really have a sense of being able to understand more complex broader queries. So query understanding and planning. There's also no kind of more sophisticated way of interacting with other services. And it's also stateless, so there's no memory. So in this setting, we have said, you know, RAG is kind of boring if it's just the simple RAG pipeline. It's really just a glorified search system on top of some retrieval methods that have been around for decades. And there's a lot of questions in tasks that naive RAG can't give an answer to. And so one thread that we've been pulling a lot on is basically figuring out how to go from simple search in naive RAG to building a general context augmented research assistant. So we'll talk about these three steps with some cool feature releases, you know, in the mix. But the first step is basically advanced data and retrieval modules. Even if you don't care about the fancy agent stuff, you need good core data quality modules to basically help you go to production. The second is advanced single agent query flows, building some agent Ragn layer on top of existing data services as tools to basically enhance the level of query understanding that your QA interface provides. So the third and this is quite interesting is this whole idea of a general multi agent task solver where you extend beyond even the capabilities of a single agent towards multi agent orchestration. So let's talk about advanced data and retrieval as a first step. The first thing is that any LLMAP these days is only as good as your data, right? If you're an ML engineer, you've heard that kind of statement many times. And so this shouldn't be net new, but it applies in the case of LLMAP development as well. Good data quality is a necessary component of any production grade LLM application. And you need that data processing layer to translate raw unstructured semi structure data into some form that's good for your LLMAP. The main components of data processing, of course, are parsing, chunking, and indexing. And let's start with parsing. So some of you might have seen these slides already, but basically the first thing that everybody needs to build some sort of proper rag pipeline is you need a good PDF parser or PowerPoint parser or some parser that can actually extract out those complex documents into a well structured representation instead of just shoving it through Pi PDF. If you have a table in a financial report and you run it through Pi PDF, it's going to destroy and collapse the information, blend the numbers and the text together, and while ends up happening is you get hallucinations. And so one of the key things about parsing is that even good parsing itself can improve performance, right? Even without advanced indexing retrieval, good parsing helps to reduce hallucinations. A simple example here is we took the CalTrain schedule, right, the weekend schedule for CalTrain, parsed it through a lot more parsed, one of our offerings, and through some well structured document parsing format, because the LLMS can actually understand well spatially laid out text. When you ask questions over it, I know the text is a little faint, this whole we find I'll share these slides later on, you're able to actually get back the correct train times for a given column versus if you shove it into Pi PDF, you get like a whole bunch of hallucinations when you ask questions over this type of data. So that's step one, you want good parsing, and you can combine this of course with advanced indexing modules to basically model heterogeneous data within a document. One announcement we're making today is we opened up LLMAParse a few months ago, it has like tens of thousands of users, tens of millions of pages processed, gotten very popular. And in general, if you're an enterprise developer that has a bucket of PDFs and wants to shove it in and not have to worry about some of these decisions, come sign up, this is basically what we're building on the LLMAP Clouds side. The next step is advanced single agent flows. So we have good data retrieval quality, or sorry, good data retrieval modules, but in the end right now we're still using a single LLM prompt call. So how do we go a little bit beyond that into something more interesting and sophisticated? We did this entire course with Andrew Ang at deep learning.ai, and we've also written extensively about this in the past few months, but basically you can layer on different components of agents on top of just a basic rag system to build something that is a lot more sophisticated and query understanding planning and tool use. So the way I like to break this down, because I all have trade offs, is on the left side you have some simple components that come with lower costs and lower latency, and then on the right you could build full blown agent systems that can operate and even work together with other agents. Some of the core agent ingredients that we see that are pretty fundamental towards building QA systems, these days include function calling and tool use, being able to actually do query planning, whether it's sequential or in some style of a DAG. And also maintain a conversation memory over time. So it's a state full service as opposed to stateless. We've pioneered this idea of a gentick rag where it's not only just, you know, rag as a single LLM prompt call where the whole responsibility is to just synthesize the information, but actually use the LLM extensively during the query understanding and processing phase, where not only are you just directly feeding the query to a vector database. So in the end, everything is just an LLM interacting with a set of data services as tools. And so this is a pretty important framework to understand, because at the end of day, you're going to have an NEP piece of LLM software, LLM is interacting with other services, whether it's a database or even other agents as tools, and you're going to need to do some sort of query planning to basically figure out how to use these tools to solve the tasks that you're given. We've also talked about agent reasoning loops, right? Probably the most stable one that we've seen so far is some sort of wild loop over function calling or react. But we've also seen fancier agent papers arise that basically deal with like DAG based planning, planning out an entire DAG of decisions or tree based planning, you know, you plan out an entire set of possible outcomes and try to optimize there. The end result is that if you're able to do this, you're able to build personalized QA systems that are capable of handling more complex questions, for instance, comparison questions across multiple documents. Being able to actually maintain the user state over time, so you can actually revisit the thing that they were looking for, being able to, for instance, look up information from not only unstructured data, but also structured data by treating everything as a data service or a tool. But, you know, there are some remaining gaps here. First of all, you know, we've kind of had some interesting discussions with other people in the community about this, but a single agent generally cannot solve an infinite set of tasks. If anyone's tried to give like a thousand tools to an agent, the agent is going to struggle and generally fail, at least with current model capabilities. And so one principle is that specialist agents tend to do better if the agent is a little bit more focused on a given task, given some input. And then the second gap is that agents are increasingly interfacing with services that, you know, maybe other agents actually. And so we might want to think about a multi agent future. So let's talk about multi agents and what that means for this idea of knowledge assistance. Multi agent task solvers. First of all, why multi agents? Well, we've mentioned this a little bit, but they offer a few benefits beyond just a single agent flow. First, they offer this idea of being able to actually specialize and operate over, you know, a focused set of tasks more reliably so that you can actually stitch together different agents that potentially can work together to solve a bigger task. Another benefit or set of benefits is on the system side by being able to have, you know, multiple copies of even like the same LLMA agent. You're able to paralyze a bunch of tasks and able to do things a lot faster. The third thing is that actually with multi agent framework, instead of having, you know, a single agent access like a thousand tools, you could potentially have each agent operate over like, you know, five to ten tools and therefore use a weaker and faster model. And so there are actually potential costs on latency savings. There are of course some fantastic multi agent frameworks that have come out in the past few months and many of you might be either using those or kind of building your own. And in general, some of the challenges in building this reliably in production include one being able to, you know, either let the agents kind of operate amongst themselves and build some sort of like unconstrained flow. Or actually being able to inject some sort of constraints between the agents you're basically explicitly forcing an agent to operate in a certain way given a certain input. The second is when you actually think about having these agents operate in production, currently the bulk of agents are implemented as functions in a Jupyter notebook. And we might want to think about defining the proper service architecture for agents in production and what that looks like. So today, you know, I'm excited to launch a preview feature of a new repo that we've been working on called llama agents. And it's an alpha feature, but basically it represents agents as micro services, right? So, you know, in addition to some of the fantastic work that a lot of these multi agent frameworks have done, the core goal of llama agents really is to think about every agent as just like a separate service and figuring out how these different services can operate together. Communicate with each other through a central API communication interface. And then also work together to solve a given task that is scalable, can handle multiple requests at once, is easy to deploy it to different types of services. And basically each agent can encapsulate a separate logic, but still communicate with each other and actually be reused across different tasks. So it really is really thinking about how do you take these agents out of a notebook and into production. And it's an idea that we've had for a while now, but we see this as a key ingredient in helping you build something that's production grade, a production grade knowledge assistant, especially as the world gets more aggrantic over time. So the core architecture here is that, you know, every agent is just represented as a separate service. You can write the agents however you want, basically, you know, with llama index with another framework as well. And we have some of the interfaces basically build custom agent. And then you're able to deploy it as a service. And basically the agents can interact with each other via some sort of message queue. And then the orchestration can happen between the agents via like a general control plane, right? We took some of the inspiration from, you know, existing resource allocators, for instance, like Kubernetes or just like other kind of like open source like systems level projects. And the orchestration can be either explicit. So you explicitly define these flows between services or it can be implicit, right? You can have some sort of LLM orchestrator just figure out what tasks to delegate to given the given the current state of things. And so one thing that I want to show you, basically, is figuring out how or just showing you how this relates to this idea of knowledge assistance, right? Because we think that multi agents are going to be a core component of this. And this is basically a demo that we whipped up showing you how to run llama agents on a basic rag pipeline. This is a pretty trivial rag pipeline. There is like a query rewriting service, right? And then also some sort of default agent that basically just does rag like search on retrieval. And you can also add in other components and services like reflection. You could have other tools as well or even a general tool service. And the core demo here is really showing that, you know, given some sort of input, they're communicating through with each other through some sort of like API protocol. And so this allows you to, for instance, launch a bunch of different client requests at once, handle, you know, tasks requests from different directions, and basically have these agents operate as like an encapsulated microservice, right? And so the query rewriting agent takes in some sort of query, processes it, rewrites it into some new query, and then, you know, the second agent will basically take in this query, do some search on retrieval, and basically output a final response. If you build a rag pipeline, all this stuff like actual logic should be relatively trivial, but the goal is to basically show you how you can turn something even that's even something that's trivial into a set of services that you can basically deploy, right? And this is just like another example that's basically a backup slide that basically again highlights the fact that you can have multiple agents, right? And they all operate and work together to basically achieve a given task. So, you know, the QR code is linked. First of all, this is in alpha mode, right? And so we're really excited to basically share this with the community. We're very public about the roadmap actually, so check out the discussions tab about what's actually in there and what's not. We're launching with dozens of dozens, basically, initial tutorials to show you how to basically build a set of like microservices that basically help you, you know, build that production grade, a gen technology assistant workflow. And there's also a repo linked that I think should be public now. You know, in general, we're pretty excited to get feedback from the community about what a general communication protocol should look like, how we basically integrate with some of the other, you know, awesome work that the community has done and basically help achieve this core mission of again building something that's production grade and a multi agent assistant. And this is just the last component, which I already mentioned, but basically if you're interested in like the data quality side of things, like let's say you don't care about agents at all and you just care about data quality. We're opening up a wait list for a lot of cloud more generally so that you're able to, you know, deal with all those decisions that I mentioned, the parsing, chunking, indexing, and ensure that, you know, your bucket of PDFs with embedded charts, tables, images is processed and parsed the right way. And if you're an enterprise developer with that use case, come talk to us. So that's basically it. Thanks for your time and hope you enjoyed it.\n"
     ]
    }
   ],
   "source": [
    "# Create the transcript using Whisper\n",
    "\n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model('base')\n",
    "result = model.transcribe(get_file(audioFileName))\n",
    "\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: delete the audio file\n",
    "\n",
    "os.remove(get_file(audioFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transcript\n",
    "\n",
    "transcriptFileName = video_file_name.split('.')[0] + '.txt'\n",
    "\n",
    "with open(get_file(transcriptFileName), \"w\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt to improve each transcript chunck using the title and the description of the video (technologies, people names, etc.)\n",
    "\n",
    "prompt_template = \"\"\"Given title and description of a video, can you check its transcript and correct it. Give back only the corrected transcript.\n",
    "\n",
    "Title: {title}\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gpt-4o model client\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the transcript in semantic chunks of maximum 500 tokens containing full sentences\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "encoding_name = 'gpt-4o'\n",
    "\n",
    "# Split the text in chunks of maximum 500 tokens with '.' as separator without using langchain\n",
    "sentences = result['text'].split('.')\n",
    "chunks = []\n",
    "chunk = ''\n",
    "chunk_number = 1\n",
    "for sentence in sentences:\n",
    "    if num_tokens_from_string(chunk + sentence, encoding_name) > 500:\n",
    "        prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "        corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        ).choices[0].message.content\n",
    "        chunks.append(corrected_chunk)\n",
    "        chunk = sentence + '. '\n",
    "        chunk_number += 1\n",
    "    else:\n",
    "        chunk += sentence + '. '\n",
    "        \n",
    "# Write the last chunk\n",
    "prompt = prompt_template.format(title=title, description=description, transcript=chunk)\n",
    "corrected_chunk = azure_openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ").choices[0].message.content\n",
    "chunks.append(corrected_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full corrected transcript and add white-lines between the chunks but not for the last chunk\n",
    "\n",
    "full_corrected_transcript = ''\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    full_corrected_transcript += chunk\n",
    "    if i < len(chunks) - 1:\n",
    "        full_corrected_transcript += '\\n\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the full corrected transcript\n",
    "\n",
    "full_corrected_transcript_file_name = video_file_name.split('.')[0] + '_corrected.txt'\n",
    "\n",
    "with open(get_file(full_corrected_transcript_file_name), \"w\") as f:\n",
    "    f.write(full_corrected_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create langchain document\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "document = Document(\n",
    "  page_content=full_corrected_transcript,\n",
    "  metadata={\n",
    "    \"title\": title,\n",
    "    \"source\": url,\n",
    "    \"description\": description,\n",
    "    \"thumbnail_url\": thumbnail_url,\n",
    "    \"page\": 0,\n",
    "    \"type\": \"video\"\n",
    "  }\n",
    ")\n",
    "\n",
    "documents = [document]  # List of documents to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast based on the video \" + title + \".\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = video_file_name.split('.')[0] + '_podcast_outline.txt'\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = video_file_name.split('.')[0] + '_podcast_script.json'\n",
    "\n",
    "with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5180:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5703:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2666:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-AndrewMultilingualNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-AriaNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = video_file_name.split('.')[0] + '_podcast.wav'\n",
    "stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
