{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU pypdf langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pdf information\n",
    "\n",
    "pdf_title = 'LoRA: Low-Rank Adaptation of Large Language Models'\n",
    "pdf_url = 'https://arxiv.org/pdf/2106.09685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA LowRank Adaptation of Large Language Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the file name without special characters\n",
    "\n",
    "pdf_filename = pdf_title.replace(':', '').replace('-', '') + '.pdf'\n",
    "print(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF as langchain document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(get_file(pdf_filename), 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the documents from the PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(get_file(pdf_filename))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata of all documents\n",
    "\n",
    "for document in documents:\n",
    "  document.metadata['title'] = pdf_title\n",
    "  document.metadata['source'] = pdf_url\n",
    "  document.metadata['description'] = ''\n",
    "  document.metadata['thumbnail_url'] = ''\n",
    "  document.metadata['type'] = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the downloaded PDF file\n",
    "\n",
    "os.remove(get_file(pdf_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast on LoRA.\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = pdf_filename.replace('.pdf', '_script.txt')\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = pdf_filename.replace('.pdf', '_script.json')\n",
    "\n",
    "with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-GuyNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-JennyNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = pdf_filename.replace('.pdf', '_podcast.wav')\n",
    "stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
