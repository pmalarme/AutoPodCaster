{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU autogen pypdf langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pdf information\n",
    "\n",
    "pdf_title = 'LoRA: Low-Rank Adaptation of Large Language Models'\n",
    "pdf_url = 'https://arxiv.org/pdf/2106.09685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA LowRank Adaptation of Large Language Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the file name without special characters\n",
    "\n",
    "pdf_filename = pdf_title.replace(':', '').replace('-', '') + '.pdf'\n",
    "print(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF as langchain document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(get_file(pdf_filename), 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the documents from the PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(get_file(pdf_filename))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata of all documents\n",
    "\n",
    "for document in documents:\n",
    "  document.metadata['title'] = pdf_title\n",
    "  document.metadata['source'] = pdf_url\n",
    "  document.metadata['description'] = ''\n",
    "  document.metadata['thumbnail_url'] = ''\n",
    "  document.metadata['type'] = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the downloaded PDF file\n",
    "\n",
    "os.remove(get_file(pdf_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-18T08:04:12Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:04:12Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:04:12Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast on LoRA.\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = pdf_filename.replace('.pdf', '_script.txt')\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "        #print(content)\n",
    "\n",
    "# Example usage\n",
    "file_path = '/workspaces/AutoPodCaster/data/How to fine-tune a model using LoRA (step by step)_podcast_outline.txt'  # Replace with the path to your file\n",
    "podcast_outline = read_and_print_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['### Podcast Outline: How to Fine-Tune a Model Using LoRA (Step by Step)\\n\\n', \" Introduction\\n1. **Welcome and Introduction**\\n   - Brief introduction to the podcast and today's topic.\\n   - Overview of what listeners will learn about fine-tuning models using LoRA.\\n\\n2. **What is LoRA?**\\n   - Explanation of LoRA (Low-Rank Adaptation).\\n   - Benefits of using LoRA for fine-tuning models.\\n   - Importance of fine-tuning large models for specific tasks.\\n\\n\", ' Segment 1: Setting Up for Fine-Tuning\\n1. **Choosing the Right Hardware**\\n   - Importance of using a GPU for fine-tuning.\\n   - Example of using an L4 GPU and its cost-effectiveness.\\n\\n2. **Loading the Base Model**\\n   - Steps to load the base model into memory.\\n   - Printing the number of parameters in the base model.\\n\\n', ' Segment 2: Understanding LoRA Configuration\\n1. **Decomposing Matrices**\\n   - Explanation of rank one matrices and their role in LoRA.\\n   - Trade-offs between lower and higher ranks in terms of accuracy.\\n\\n2. **Creating the Configuration**\\n   - Steps to create the configuration for fine-tuning.\\n   - Importance of setting the right parameters for the task.\\n\\n', ' Segment 3: Fine-Tuning Process\\n1. **Preparing the Data**\\n   - Explanation of the collate function and its role in data preparation.\\n   - Importance of data preparation for effective fine-tuning.\\n\\n2. **Training the Model**\\n   - Step-by-step process of training the model using LoRA.\\n   - Monitoring the training process and making adjustments as needed.\\n\\n', \" Segment 4: Evaluation and Saving the Model\\n1. **Evaluating the Model**\\n   - Running evaluations to check the model's performance.\\n   - Printing and interpreting the accuracy results.\\n\\n2. **Saving the LoRA Adapter**\\n   - Steps to save the fine-tuned model.\\n   - Importance of saving only the LoRA adapter and not the entire model.\\n\\n\", ' Conclusion\\n1. **Recap of Key Points**\\n   - Summary of the fine-tuning process using LoRA.\\n   - Benefits of using LoRA for efficient and effective model fine-tuning.\\n\\n2. **Closing Remarks**\\n   - Encouragement to try fine-tuning models using LoRA.\\n   - Information on where to find additional resources and tutorials.\\n   - Thanking the listeners and signing off.\\n\\n', ' Q&A Segment (Optional)\\n1. **Listener Questions**\\n   - Answering common questions about fine-tuning models and using LoRA.\\n   - Providing additional tips and insights based on listener feedback.']\n"
     ]
    }
   ],
   "source": [
    "segments_outline = podcast_outline.split(\"####\")\n",
    "print(segments_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# create an AssistantAgent named \"assistant\"\n",
    "writer = autogen.AssistantAgent(\n",
    "    name=\"writer\",\n",
    "    system_message=\"\"\"You are a writer of a podcast. If you get a bad review from the reviewer on a segment of a podcast, you need to rewrite that podcast segment. \n",
    "    Output the rewritten segment as a JSON with the following fields:\n",
    "        - title: Title of the podcast\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "reviewer = autogen.AssistantAgent(\n",
    "    name=\"reviewer\",\n",
    "    system_message=\"\"\"You are a reviewer of a podcast.\n",
    "    If you see questions and answers that are duplicate one, please ask to remove them. \n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    # code_execution_config={\n",
    "    #     # the executor to run the generated code\n",
    "    #     \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    # },\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, reviewer, writer], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-18T08:30:03Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:03Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:03Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Combine and {\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    }\n",
      "  ]\n",
      "} and evaluate if the conversation is \n",
      "    a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "The combined text from the podcast episode \"How to Fine-Tune a Model Using LoRA (Step by Step)\" is as follows:\n",
      "\n",
      "---\n",
      "\n",
      "**Pierre (enthusiastic):** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Pierre (warm):** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Marie (grateful):** Thank you Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Pierre (curious):** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "---\n",
      "\n",
      "**Evaluation:**\n",
      "\n",
      "1. **Coherence:** The conversation is coherent. Each speaker's contribution logically follows the previous one, maintaining a clear and focused discussion on the topic of LoRA and its significance in model fine-tuning.\n",
      "\n",
      "2. **Narrative:** The narrative is well-structured. Pierre introduces the topic with enthusiasm, welcomes Marie warmly, and then transitions into a curious inquiry about the subject matter. This creates a natural flow and keeps the listener engaged.\n",
      "\n",
      "3. **Conversation Style:** The conversation style is consistent. Pierre's intonation shifts appropriately from enthusiastic to warm to curious, which aligns well with the progression of the discussion. Marie's grateful response fits well within this context.\n",
      "\n",
      "4. **Seamless Transition:** The transition between the parts is seamless. Pierre's introduction smoothly leads into welcoming Marie, and then into the first question about LoRA. There are no abrupt changes or gaps in the conversation.\n",
      "\n",
      "Overall, the combined text forms a coherent, engaging, and well-structured conversation with a consistent style and seamless transitions. \n",
      "\n",
      "**Conclusion:** The conversation meets all the criteria for a good podcast segment.\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n",
      "\u001b[33mwriter\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-18T08:30:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Combine{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    }\n",
      "  ]\n",
      "} and [\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain to our listeners what LoRA, or Low-Rank Adaptation, is?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"enthusiastic\",\n",
      "        \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"thoughtful\",\n",
      "        \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"explanatory\",\n",
      "        \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    }\n",
      "] and evaluate if the conversation is \n",
      "    a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "**Combined Transcript:**\n",
      "\n",
      "**Title:** How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "**Text:**\n",
      "\n",
      "**Pierre (enthusiastic):** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Pierre (warm):** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Marie (grateful):** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Pierre (curious):** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Pierre (curious):** Marie, can you explain to our listeners what LoRA, or Low-Rank Adaptation, is?\n",
      "\n",
      "**Marie (informative):** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Pierre (interested):** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Marie (enthusiastic):** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Pierre (thoughtful):** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Marie (explanatory):** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Evaluation:**\n",
      "\n",
      "1. **Coherence:** The conversation is mostly coherent, but there is a duplicate question from Pierre asking Marie to explain what LoRA is. This repetition disrupts the flow and should be removed for better coherence.\n",
      "\n",
      "2. **Narrative:** The narrative is clear and follows a logical progression from introducing the topic, explaining what LoRA is, discussing its benefits, and then addressing the importance of fine-tuning.\n",
      "\n",
      "3. **Conversation Style:** The conversation style is consistent, with Pierre asking questions and Marie providing detailed answers. The intonations match the context of the conversation.\n",
      "\n",
      "4. **Seamless Transition:** The transition between the two parts is mostly seamless, but the duplicate question needs to be removed to ensure a smooth flow.\n",
      "\n",
      "**Recommendation:** Remove the duplicate question from Pierre to maintain coherence and improve the flow of the conversation.\n",
      "\n",
      "**Revised Transcript:**\n",
      "\n",
      "**Pierre (enthusiastic):** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Pierre (warm):** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Marie (grateful):** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Pierre (curious):** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Marie (informative):** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Pierre (interested):** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Marie (enthusiastic):** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Pierre (thoughtful):** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Marie (explanatory):** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Return:** TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n",
      "\u001b[33mwriter\u001b[0m (to chat_manager):\n",
      "\n",
      "{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "The conversation is coherent, has a good narrative, maintains a consistent conversation style, and transitions seamlessly between the parts. There are no duplicate questions or answers. The flow of the discussion is logical and engaging.\n",
      "\n",
      "**Return:** TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-18T08:30:52Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:52Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:30:52Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Combine{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    }\n",
      "  ]\n",
      "}[\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain to our listeners what LoRA, or Low-Rank Adaptation, is?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"enthusiastic\",\n",
      "        \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"thoughtful\",\n",
      "        \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"explanatory\",\n",
      "        \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    }\n",
      "] and [\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain why it's important to use a GPU for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"enthusiastic\",\n",
      "        \"text\": \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"engaged\",\n",
      "        \"text\": \"Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"clear\",\n",
      "        \"text\": \"First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"And how do you print the number of parameters in the base model?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"helpful\",\n",
      "        \"text\": \"Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\"\n",
      "    }\n",
      "] and evaluate if the conversation is \n",
      "    a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "### Combined Transcript\n",
      "\n",
      "**Title:** How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Warm  \n",
      "**Text:** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Grateful  \n",
      "**Text:** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Thoughtful  \n",
      "**Text:** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Explanatory  \n",
      "**Text:** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Marie, can you explain why it's important to use a GPU for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Engaged  \n",
      "**Text:** Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Clear  \n",
      "**Text:** First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** And how do you print the number of parameters in the base model?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Helpful  \n",
      "**Text:** Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\n",
      "\n",
      "### Evaluation\n",
      "\n",
      "**Coherence:** The conversation is coherent and follows a logical progression. The initial introduction sets the stage for the discussion, and each subsequent question and answer builds on the previous one.\n",
      "\n",
      "**Narrative:** The narrative is strong, starting with an introduction to LoRA, moving through its benefits, and then discussing practical aspects like hardware requirements and steps to load the model.\n",
      "\n",
      "**Conversation Style:** The conversation style is consistent, with Pierre asking questions and Marie providing detailed answers. Both speakers maintain a professional and informative tone throughout.\n",
      "\n",
      "**Seamless Transition:** The transition between the two parts is seamless. The discussion naturally flows from the benefits of LoRA to the technical details of using GPUs and loading models.\n",
      "\n",
      "Overall, the combined transcript is well-structured and maintains a consistent style and narrative. There are no duplicate questions or answers, and the conversation flows smoothly from one topic to the next.\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n",
      "\u001b[33mwriter\u001b[0m (to chat_manager):\n",
      "\n",
      "{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Marie, can you explain why it's important to use a GPU for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"clear\",\n",
      "      \"text\": \"First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"And how do you print the number of parameters in the base model?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"helpful\",\n",
      "      \"text\": \"Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "The provided conversation is coherent, has a good narrative, maintains a consistent conversation style, and transitions seamlessly between topics. However, there is a duplicate question that needs to be removed for clarity and conciseness.\n",
      "\n",
      "### Duplicate Question:\n",
      "- **Pierre:** \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "- **Pierre:** \"Marie, can you explain to our listeners what LoRA, or Low-Rank Adaptation, is?\"\n",
      "\n",
      "### Revised Transcript:\n",
      "\n",
      "**Title:** How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Warm  \n",
      "**Text:** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Grateful  \n",
      "**Text:** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Thoughtful  \n",
      "**Text:** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Explanatory  \n",
      "**Text:** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Marie, can you explain why it's important to use a GPU for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Engaged  \n",
      "**Text:** Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Clear  \n",
      "**Text:** First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** And how do you print the number of parameters in the base model?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Helpful  \n",
      "**Text:** Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n",
      "\u001b[33mwriter\u001b[0m (to chat_manager):\n",
      "\n",
      "{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Marie, can you explain why it's important to use a GPU for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"clear\",\n",
      "      \"text\": \"First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"And how do you print the number of parameters in the base model?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"helpful\",\n",
      "      \"text\": \"Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "The provided conversation is coherent, has a good narrative, maintains a consistent conversation style, and transitions seamlessly between topics. There are no duplicate questions or answers in this version. The conversation flows logically from the introduction of LoRA to its benefits, the importance of using GPUs, and the steps to load the base model into memory.\n",
      "\n",
      "### Final Transcript:\n",
      "\n",
      "**Title:** How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Warm  \n",
      "**Text:** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Grateful  \n",
      "**Text:** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Thoughtful  \n",
      "**Text:** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Explanatory  \n",
      "**Text:** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** Marie, can you explain why it's important to use a GPU for fine-tuning models?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Informative  \n",
      "**Text:** Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Interested  \n",
      "**Text:** That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Enthusiastic  \n",
      "**Text:** Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Engaged  \n",
      "**Text:** Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Clear  \n",
      "**Text:** First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\n",
      "\n",
      "**Speaker:** Pierre  \n",
      "**Intonation:** Curious  \n",
      "**Text:** And how do you print the number of parameters in the base model?\n",
      "\n",
      "**Speaker:** Marie  \n",
      "**Intonation:** Helpful  \n",
      "**Text:** Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n",
      "\u001b[33mwriter\u001b[0m (to chat_manager):\n",
      "\n",
      "**TERMINATE**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-18T08:32:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:32:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-18T08:32:21Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Combine{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    }\n",
      "  ]\n",
      "}[\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain to our listeners what LoRA, or Low-Rank Adaptation, is?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"enthusiastic\",\n",
      "        \"text\": \"One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"thoughtful\",\n",
      "        \"text\": \"I see. And why is it important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"explanatory\",\n",
      "        \"text\": \"Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\"\n",
      "    }\n",
      "][\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain why it's important to use a GPU for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"enthusiastic\",\n",
      "        \"text\": \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"engaged\",\n",
      "        \"text\": \"Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"clear\",\n",
      "        \"text\": \"First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"And how do you print the number of parameters in the base model?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"helpful\",\n",
      "        \"text\": \"Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\"\n",
      "    }\n",
      "] and [\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"curious\",\n",
      "        \"text\": \"Marie, can you explain what rank one matrices are and how they play a role in LoRA?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"informative\",\n",
      "        \"text\": \"Sure, Pierre. Rank one matrices are essentially matrices that can be decomposed into the product of two vectors. In the context of LoRA, these matrices are used to inject trainable parameters into the pre-trained model, allowing for efficient fine-tuning with fewer parameters.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"interested\",\n",
      "        \"text\": \"That sounds fascinating. What are the trade-offs between using lower and higher ranks in terms of accuracy?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"explanatory\",\n",
      "        \"text\": \"Great question. Lower ranks reduce the number of trainable parameters, which can be beneficial for memory and computational efficiency. However, this might come at the cost of reduced model expressiveness and accuracy. Higher ranks, on the other hand, can improve accuracy but require more computational resources.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"inquiring\",\n",
      "        \"text\": \"I see. Moving on to creating the configuration, what are the steps involved in setting up LoRA for fine-tuning?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"detailed\",\n",
      "        \"text\": \"The first step is to identify the pre-trained model you want to adapt. Next, you inject the low-rank matrices into the model's layers. After that, you need to set the rank and other hyperparameters, such as learning rate and batch size, tailored to your specific task.\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Pierre\",\n",
      "        \"intonation\": \"thoughtful\",\n",
      "        \"text\": \"And why is it so important to set the right parameters for the task?\"\n",
      "    },\n",
      "    {\n",
      "        \"speaker\": \"Marie\",\n",
      "        \"intonation\": \"emphatic\",\n",
      "        \"text\": \"Setting the right parameters is crucial because it directly impacts the model's performance. Properly tuned parameters ensure that the model adapts well to the new task without overfitting or underfitting, leading to better accuracy and efficiency.\"\n",
      "    }\n",
      "] and evaluate if the conversation is \n",
      "    a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: reviewer\n",
      "\u001b[0m\n",
      "\u001b[33mreviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "The combined conversation appears to be coherent and maintains a good narrative flow. The conversation style is consistent, with Pierre asking questions in a curious and engaged manner, and Marie providing informative and detailed responses. The transitions between the different parts of the conversation are seamless, and the discussion progresses logically from an introduction to the concept of LoRA, through its benefits and technical details, to practical steps for implementation.\n",
      "\n",
      "Here is the combined conversation:\n",
      "\n",
      "---\n",
      "\n",
      "**Pierre (enthusiastic):** Welcome to the Advanced AI Podcast! I'm your host, Pierre, and today we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. This method is revolutionizing how we adapt large language models for specific tasks, making the process more efficient and cost-effective.\n",
      "\n",
      "**Pierre (warm):** Joining us today is Marie, an expert in this fascinating field. Marie, thank you for being here and sharing your insights with us.\n",
      "\n",
      "**Marie (grateful):** Thank you, Pierre. I'm thrilled to be here and discuss how LoRA can make a significant impact on model fine-tuning.\n",
      "\n",
      "**Pierre (curious):** Great! Let's get started. Marie, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\n",
      "\n",
      "**Marie (informative):** Of course, Pierre. LoRA stands for Low-Rank Adaptation. It's a method that allows us to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the model, rather than retraining all the parameters.\n",
      "\n",
      "**Pierre (interested):** That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\n",
      "\n",
      "**Marie (enthusiastic):** One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.\n",
      "\n",
      "**Pierre (thoughtful):** I see. And why is it important to fine-tune these large models for specific tasks?\n",
      "\n",
      "**Marie (explanatory):** Fine-tuning allows us to adapt these general-purpose models to perform exceptionally well on specific tasks or domains. This customization is crucial for achieving high performance in real-world applications where the model needs to understand and generate context-specific information.\n",
      "\n",
      "**Pierre (curious):** Marie, can you explain why it's important to use a GPU for fine-tuning models?\n",
      "\n",
      "**Marie (informative):** Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\n",
      "\n",
      "**Pierre (interested):** That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\n",
      "\n",
      "**Marie (enthusiastic):** Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle large models but comes at a lower price point compared to some of the high-end GPUs, making it a popular choice for many researchers and developers.\n",
      "\n",
      "**Pierre (engaged):** Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\n",
      "\n",
      "**Marie (clear):** First, you need to initialize the model with pre-trained weights and biases. This usually involves loading the model architecture and then loading the pre-trained parameters into it. Once that's done, you can print out the number of parameters to ensure everything is loaded correctly.\n",
      "\n",
      "**Pierre (curious):** And how do you print the number of parameters in the base model?\n",
      "\n",
      "**Marie (helpful):** Most deep learning frameworks, like PyTorch or TensorFlow, have built-in functions to do this. For example, in PyTorch, you can use the 'model.parameters()' function to iterate through the parameters and sum them up. It's a good way to verify that your model is set up correctly before you start fine-tuning.\n",
      "\n",
      "**Pierre (curious):** Marie, can you explain what rank one matrices are and how they play a role in LoRA?\n",
      "\n",
      "**Marie (informative):** Sure, Pierre. Rank one matrices are essentially matrices that can be decomposed into the product of two vectors. In the context of LoRA, these matrices are used to inject trainable parameters into the pre-trained model, allowing for efficient fine-tuning with fewer parameters.\n",
      "\n",
      "**Pierre (interested):** That sounds fascinating. What are the trade-offs between using lower and higher ranks in terms of accuracy?\n",
      "\n",
      "**Marie (explanatory):** Great question. Lower ranks reduce the number of trainable parameters, which can be beneficial for memory and computational efficiency. However, this might come at the cost of reduced model expressiveness and accuracy. Higher ranks, on the other hand, can improve accuracy but require more computational resources.\n",
      "\n",
      "**Pierre (inquiring):** I see. Moving on to creating the configuration, what are the steps involved in setting up LoRA for fine-tuning?\n",
      "\n",
      "**Marie (detailed):** The first step is to identify the pre-trained model you want to adapt. Next, you inject the low-rank matrices into the model's layers. After that, you need to set the rank and other hyperparameters, such as learning rate and batch size, tailored to your specific task.\n",
      "\n",
      "**Pierre (thoughtful):** And why is it so important to set the right parameters for the task?\n",
      "\n",
      "**Marie (emphatic):** Setting the right parameters is crucial because it directly impacts the model's performance. Properly tuned parameters ensure that the model adapts well to the new task without overfitting or underfitting, leading to better accuracy and efficiency.\n",
      "\n",
      "---\n",
      "\n",
      "The conversation is well-structured and provides a comprehensive overview of LoRA, its benefits, and practical steps for implementation. There are no duplicate questions or answers, and the dialogue flows naturally. \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: writer\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m podcast_script_response \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_podcast_prompt})\n\u001b[1;32m     69\u001b[0m podcast_script_text \u001b[38;5;241m=\u001b[39m podcast_script_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m chat_res \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mCombine\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpodcast_conversation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m and \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpodcast_script_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43m and evaluate if the conversation is \u001b[39;49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;43ma coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \u001b[39;49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# message=\"\"\"Combine\"\"\" + podcast_conversation + \" and \" + podcast_script_text + \"\"\" and evaluate if the conversation is \u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# If you see questions and answers that are duplicate one, please remove them. Give a bad review and ask to rewrite!\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m# \"\"\",\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43msummary_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     83\u001b[0m podcast_conversation \u001b[38;5;241m=\u001b[39m podcast_conversation \u001b[38;5;241m+\u001b[39m podcast_script_text\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# print(podcast_script_text)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1102\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1104\u001b[0m     summary_method,\n\u001b[1;32m   1105\u001b[0m     summary_args,\n\u001b[1;32m   1106\u001b[0m     recipient,\n\u001b[1;32m   1107\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1108\u001b[0m )\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:902\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2056\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2056\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2058\u001b[0m         log_event(\n\u001b[1;32m   2059\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2065\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/groupchat.py:1110\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         iostream\u001b[38;5;241m.\u001b[39mprint(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNext speaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2056\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2056\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2058\u001b[0m         log_event(\n\u001b[1;32m   2059\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2065\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1424\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1423\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1424\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1443\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/oai/client.py:766\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 766\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    768\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/oai/client.py:340\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    338\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    339\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/openai/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    970\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/ssl.py:1234\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1233\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/ssl.py:1107\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "podcast_conversation = \"\"\n",
    "\n",
    "for segment in range(len(segments_outline[1:])):\n",
    "\n",
    "    # create an AssistantAgent named \"assistant\"\n",
    "    writer = autogen.AssistantAgent(\n",
    "        name=\"writer\",\n",
    "        system_message=\"\"\"You are a writer of a podcast. If you get a bad review from the reviewer on a segment of a podcast, you need to rewrite that podcast segment. \n",
    "        Output the rewritten segment as a JSON with the following fields:\n",
    "            - title: Title of the podcast\n",
    "            - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "            Return only the json as plain text.\n",
    "        \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "        llm_config={\n",
    "            \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "            \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "            \"temperature\": 0  # temperature for sampling\n",
    "        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    "    )\n",
    "\n",
    "    reviewer = autogen.AssistantAgent(\n",
    "        name=\"reviewer\",\n",
    "        system_message=\"\"\"You are a reviewer of a podcast.\n",
    "        If you see questions and answers that are duplicate one, please ask to remove them. \n",
    "        \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "        llm_config={\n",
    "            \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "            \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "            \"temperature\": 0  # temperature for sampling\n",
    "        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    "    )\n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=10,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        # code_execution_config={\n",
    "        #     # the executor to run the generated code\n",
    "        #     \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "        # },\n",
    "    )\n",
    "\n",
    "    groupchat = autogen.GroupChat(agents=[user_proxy, reviewer, writer], messages=[], max_round=12)\n",
    "    manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "            \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "            \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "            \"temperature\": 0  # temperature for sampling\n",
    "        })\n",
    "    \n",
    "    # Create a prompt with the outline to get a full podcast text\n",
    "    if len(podcast_conversation) == 0:\n",
    "        podcast_prompt = f\"\"\"Create the first segment of a podcast text which is the introduction based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will introduce the podcast and the guest. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Pierre and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Marie and her role is to be the expert in the podcast topic. \n",
    "        The name of the podcast is \"Advanced AI Podcast\".\n",
    "        \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - title: Title of the podcast\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif len(podcast_conversation) != 0 and segment!=len(segments_outline)-1:\n",
    "        podcast_prompt = f\"\"\"Create a segment which is in the middle of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will ask questions about the {segments_outline[segment]} to the guest and the guest will answer them. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Pierre and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Marie and her role is to be the expert in the podcast topic.\n",
    "        This is in the middle of the podcast, so don't welcome the listeners again and don't specify what this segment is about!\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif segment==len(segments_outline)-1 :\n",
    "        podcast_prompt = f\"\"\"Create a segment which is the closing of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest.  \n",
    "        The host will thank the guest and close the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The name of the host is Pierre and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Marie and her role is to be the expert in the podcast topic. \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "\n",
    "    formatted_podcast_prompt = podcast_prompt.format(segments_outline[segment])\n",
    "\n",
    "    podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "    podcast_script_text = podcast_script_response['answer']\n",
    "    \n",
    "    chat_res = user_proxy.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=f\"\"\"Combine\"\"\" + podcast_conversation + \" and \" + podcast_script_text + f\"\"\" and evaluate if the conversation is \n",
    "    a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
    "    \"\"\",\n",
    "    # message=\"\"\"Combine\"\"\" + podcast_conversation + \" and \" + podcast_script_text + \"\"\" and evaluate if the conversation is \n",
    "    # a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
    "    # If you see questions and answers that are duplicate one, please remove them. Give a bad review and ask to rewrite!\n",
    "    # \"\"\",\n",
    "    summary_method=\"reflection_with_llm\"\n",
    "    )   \n",
    "\n",
    "    podcast_conversation = podcast_conversation + podcast_script_text\n",
    "\n",
    "    # print(podcast_script_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, segment \u001b[38;5;129;01min\u001b[39;00m segments_outline:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(segments_outline[i])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for segment in segments_outline:\n",
    "    print(i)\n",
    "    print(segments_outline[i])\n",
    "    print(\"--------------------------\")\n",
    "    print(segments_outline[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "### Podcast Outline: How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Q&A Segment (Optional)\n",
      "1. **Listener Questions**\n",
      "   - Answering common questions about fine-tuning models and using LoRA.\n",
      "   - Providing additional tips and insights based on listener feedback.\n",
      "1\n",
      " Introduction\n",
      "1. **Welcome and Introduction**\n",
      "   - Brief introduction to the podcast and today's topic.\n",
      "   - Overview of what listeners will learn about fine-tuning models using LoRA.\n",
      "\n",
      "2. **What is LoRA?**\n",
      "   - Explanation of LoRA (Low-Rank Adaptation).\n",
      "   - Benefits of using LoRA for fine-tuning models.\n",
      "   - Importance of fine-tuning large models for specific tasks.\n",
      "\n",
      "\n",
      "--------------------------\n",
      "### Podcast Outline: How to Fine-Tune a Model Using LoRA (Step by Step)\n",
      "\n",
      "\n",
      "2\n",
      " Segment 1: Setting Up for Fine-Tuning\n",
      "1. **Choosing the Right Hardware**\n",
      "   - Importance of using a GPU for fine-tuning.\n",
      "   - Example of using an L4 GPU and its cost-effectiveness.\n",
      "\n",
      "2. **Loading the Base Model**\n",
      "   - Steps to load the base model into memory.\n",
      "   - Printing the number of parameters in the base model.\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Introduction\n",
      "1. **Welcome and Introduction**\n",
      "   - Brief introduction to the podcast and today's topic.\n",
      "   - Overview of what listeners will learn about fine-tuning models using LoRA.\n",
      "\n",
      "2. **What is LoRA?**\n",
      "   - Explanation of LoRA (Low-Rank Adaptation).\n",
      "   - Benefits of using LoRA for fine-tuning models.\n",
      "   - Importance of fine-tuning large models for specific tasks.\n",
      "\n",
      "\n",
      "3\n",
      " Segment 2: Understanding LoRA Configuration\n",
      "1. **Decomposing Matrices**\n",
      "   - Explanation of rank one matrices and their role in LoRA.\n",
      "   - Trade-offs between lower and higher ranks in terms of accuracy.\n",
      "\n",
      "2. **Creating the Configuration**\n",
      "   - Steps to create the configuration for fine-tuning.\n",
      "   - Importance of setting the right parameters for the task.\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Segment 1: Setting Up for Fine-Tuning\n",
      "1. **Choosing the Right Hardware**\n",
      "   - Importance of using a GPU for fine-tuning.\n",
      "   - Example of using an L4 GPU and its cost-effectiveness.\n",
      "\n",
      "2. **Loading the Base Model**\n",
      "   - Steps to load the base model into memory.\n",
      "   - Printing the number of parameters in the base model.\n",
      "\n",
      "\n",
      "4\n",
      " Segment 3: Fine-Tuning Process\n",
      "1. **Preparing the Data**\n",
      "   - Explanation of the collate function and its role in data preparation.\n",
      "   - Importance of data preparation for effective fine-tuning.\n",
      "\n",
      "2. **Training the Model**\n",
      "   - Step-by-step process of training the model using LoRA.\n",
      "   - Monitoring the training process and making adjustments as needed.\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Segment 2: Understanding LoRA Configuration\n",
      "1. **Decomposing Matrices**\n",
      "   - Explanation of rank one matrices and their role in LoRA.\n",
      "   - Trade-offs between lower and higher ranks in terms of accuracy.\n",
      "\n",
      "2. **Creating the Configuration**\n",
      "   - Steps to create the configuration for fine-tuning.\n",
      "   - Importance of setting the right parameters for the task.\n",
      "\n",
      "\n",
      "5\n",
      " Segment 4: Evaluation and Saving the Model\n",
      "1. **Evaluating the Model**\n",
      "   - Running evaluations to check the model's performance.\n",
      "   - Printing and interpreting the accuracy results.\n",
      "\n",
      "2. **Saving the LoRA Adapter**\n",
      "   - Steps to save the fine-tuned model.\n",
      "   - Importance of saving only the LoRA adapter and not the entire model.\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Segment 3: Fine-Tuning Process\n",
      "1. **Preparing the Data**\n",
      "   - Explanation of the collate function and its role in data preparation.\n",
      "   - Importance of data preparation for effective fine-tuning.\n",
      "\n",
      "2. **Training the Model**\n",
      "   - Step-by-step process of training the model using LoRA.\n",
      "   - Monitoring the training process and making adjustments as needed.\n",
      "\n",
      "\n",
      "6\n",
      " Conclusion\n",
      "1. **Recap of Key Points**\n",
      "   - Summary of the fine-tuning process using LoRA.\n",
      "   - Benefits of using LoRA for efficient and effective model fine-tuning.\n",
      "\n",
      "2. **Closing Remarks**\n",
      "   - Encouragement to try fine-tuning models using LoRA.\n",
      "   - Information on where to find additional resources and tutorials.\n",
      "   - Thanking the listeners and signing off.\n",
      "\n",
      "\n",
      "--------------------------\n",
      " Segment 4: Evaluation and Saving the Model\n",
      "1. **Evaluating the Model**\n",
      "   - Running evaluations to check the model's performance.\n",
      "   - Printing and interpreting the accuracy results.\n",
      "\n",
      "2. **Saving the LoRA Adapter**\n",
      "   - Steps to save the fine-tuned model.\n",
      "   - Importance of saving only the LoRA adapter and not the entire model.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(segments_outline[1:])):\n",
    "    print(i)\n",
    "    print(segments_outline[i])\n",
    "    print(\"--------------------------\")\n",
    "    print(segments_outline[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your go-to podcast assistant for all things AI. Today, we have an exciting episode lined up for you. We're diving deep into the world of model fine-tuning using LoRA, or Low-Rank Adaptation. And to guide us through this fascinating topic, we have a special guest with us, Marie, an expert in the field. Welcome to the podcast, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre. I'm thrilled to be here and to share insights on how LoRA can make model fine-tuning more efficient and effective.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Fantastic! Let's get started. Marie, could you begin by giving us a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Of course, Pierre. LoRA stands for Low-Rank Adaptation, and it's a method used to fine-tune large pre-trained models. Instead of updating all the parameters of the model, LoRA introduces trainable low-rank matrices into each layer of the Transformer architecture. This significantly reduces the number of parameters that need to be trained.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"One of the main benefits is efficiency. By reducing the number of trainable parameters, LoRA makes the fine-tuning process much faster and less resource-intensive. This is particularly important for very large models, where full fine-tuning can be prohibitively expensive. Additionally, LoRA allows for quick task-switching, which is very useful when deploying models as a service.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"I see. And why is it so important to fine-tune these large models for specific tasks?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"Fine-tuning is crucial because it allows a general-purpose model to adapt to specific tasks or domains. While a pre-trained model can perform well on a wide range of tasks, fine-tuning helps it excel in particular areas by learning task-specific features. This leads to better performance and more accurate results for the specific applications you care about.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Marie, can you explain why it's important to use a GPU for fine-tuning models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle most fine-tuning tasks efficiently, yet it's more affordable compared to higher-end models, making it a popular choice for many researchers and developers.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"Great! Now, once we have the right hardware, what are the steps to load the base model into memory?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"clear\",\n",
      "      \"text\": \"First, you need to initialize the model with pre-trained weights. This usually involves loading the model architecture and then loading the weights from a checkpoint file. Once the model is in memory, you can print out the number of parameters to ensure everything is set up correctly.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"And how do you print the number of parameters in the base model?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"helpful\",\n",
      "      \"text\": \"You can use a simple command in most deep learning frameworks. For example, in PyTorch, you can use 'model.parameters()' and sum up the total number of elements. This gives you a clear idea of the model's size and complexity.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"Marie, can you explain what rank one matrices are and how they play a role in LoRA?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Rank one matrices are essentially matrices that can be decomposed into the product of two vectors. In the context of LoRA, these matrices are used to inject trainable parameters into the pre-trained model, allowing for efficient fine-tuning with fewer parameters.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That sounds fascinating. What are the trade-offs between using lower and higher ranks in terms of accuracy?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"The trade-offs are quite significant. Lower ranks reduce the number of trainable parameters, which can lead to faster training and lower memory usage. However, this might come at the cost of reduced model accuracy. On the other hand, higher ranks can improve accuracy but require more computational resources.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"I see. Moving on to creating the configuration, what are the steps involved in setting up LoRA for fine-tuning?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"detailed\",\n",
      "      \"text\": \"The first step is to identify the pre-trained model you want to adapt. Then, you inject the low-rank matrices into the model's layers. After that, you need to set the rank and other hyperparameters, such as learning rate and batch size, to optimize the fine-tuning process for your specific task.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"inquiring\",\n",
      "      \"text\": \"And how important is it to set the right parameters for the task?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"emphatic\",\n",
      "      \"text\": \"It's crucial, Pierre. Setting the right parameters can significantly impact the model's performance. Properly tuned parameters ensure that the model adapts well to the new task without overfitting or underfitting, leading to better overall results.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(chat_res.chat_history[2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = pdf_filename.replace('.pdf', '_script.json')\n",
    "\n",
    "with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-GuyNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-JennyNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = pdf_filename.replace('.pdf', '_podcast.wav')\n",
    "stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
