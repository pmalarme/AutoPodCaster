{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU autogen pypdf langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pdf information\n",
    "\n",
    "pdf_title = 'LoRA: Low-Rank Adaptation of Large Language Models'\n",
    "pdf_url = 'https://arxiv.org/pdf/2106.09685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA LowRank Adaptation of Large Language Models.pdf\n"
     ]
    }
   ],
   "source": [
    "# Define the file name without special characters\n",
    "\n",
    "pdf_filename = pdf_title.replace(':', '').replace('-', '') + '.pdf'\n",
    "print(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF as langchain document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(get_file(pdf_filename), 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7c313933d2e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Create the documents from the PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(get_file(pdf_filename))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata of all documents\n",
    "\n",
    "for document in documents:\n",
    "  document.metadata['title'] = pdf_title\n",
    "  document.metadata['source'] = pdf_url\n",
    "  document.metadata['description'] = ''\n",
    "  document.metadata['thumbnail_url'] = ''\n",
    "  document.metadata['type'] = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7bda92f7c140>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7bda92f7d6a0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-3-large', openai_api_version='2024-02-15-preview', openai_api_base=None, openai_api_type='azure', openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=2048, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True, azure_endpoint='https://ipej-hackathon24-aoi.openai.azure.com/', azure_ad_token=None, azure_ad_token_provider=None, validate_base_url=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_openai_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the downloaded PDF file\n",
    "\n",
    "os.remove(get_file(pdf_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:24:10Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:10Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:10Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    }
   ],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast on LoRA.\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = pdf_filename.replace('.pdf', '_script.txt')\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "        #print(content)\n",
    "\n",
    "# Example usage\n",
    "file_path = '/workspaces/AutoPodCaster/data/How to fine-tune a model using LoRA (step by step)_podcast_outline.txt'  # Replace with the path to your file\n",
    "podcast_outline = read_and_print_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['### Podcast Outline: How to Fine-Tune a Model Using LoRA (Step by Step)\\n\\n', \" Introduction\\n1. **Welcome and Introduction**\\n   - Brief introduction to the podcast and today's topic.\\n   - Overview of what listeners will learn about fine-tuning models using LoRA.\\n\\n2. **What is LoRA?**\\n   - Explanation of LoRA (Low-Rank Adaptation).\\n   - Benefits of using LoRA for fine-tuning models.\\n   - Importance of fine-tuning large models for specific tasks.\\n\\n\", ' Segment 1: Setting Up for Fine-Tuning\\n1. **Choosing the Right Hardware**\\n   - Importance of using a GPU for fine-tuning.\\n   - Example of using an L4 GPU and its cost-effectiveness.\\n\\n2. **Loading the Base Model**\\n   - Steps to load the base model into memory.\\n   - Printing the number of parameters in the base model.\\n\\n', ' Segment 2: Understanding LoRA Configuration\\n1. **Decomposing Matrices**\\n   - Explanation of rank one matrices and their role in LoRA.\\n   - Trade-offs between lower and higher ranks in terms of accuracy.\\n\\n2. **Creating the Configuration**\\n   - Steps to create the configuration for fine-tuning.\\n   - Importance of setting the right parameters for the task.\\n\\n', ' Segment 3: Fine-Tuning Process\\n1. **Preparing the Data**\\n   - Explanation of the collate function and its role in data preparation.\\n   - Importance of data preparation for effective fine-tuning.\\n\\n2. **Training the Model**\\n   - Step-by-step process of training the model using LoRA.\\n   - Monitoring the training process and making adjustments as needed.\\n\\n', \" Segment 4: Evaluation and Saving the Model\\n1. **Evaluating the Model**\\n   - Running evaluations to check the model's performance.\\n   - Printing and interpreting the accuracy results.\\n\\n2. **Saving the LoRA Adapter**\\n   - Steps to save the fine-tuned model.\\n   - Importance of saving only the LoRA adapter and not the entire model.\\n\\n\", ' Conclusion\\n1. **Recap of Key Points**\\n   - Summary of the fine-tuning process using LoRA.\\n   - Benefits of using LoRA for efficient and effective model fine-tuning.\\n\\n2. **Closing Remarks**\\n   - Encouragement to try fine-tuning models using LoRA.\\n   - Information on where to find additional resources and tutorials.\\n   - Thanking the listeners and signing off.\\n\\n', ' Q&A Segment (Optional)\\n1. **Listener Questions**\\n   - Answering common questions about fine-tuning models and using LoRA.\\n   - Providing additional tips and insights based on listener feedback.']\n"
     ]
    }
   ],
   "source": [
    "segments_outline = podcast_outline.split(\"####\")\n",
    "print(segments_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What date is today? Compare the year-to-date gain for META and TESLA.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "To compare the year-to-date (YTD) gain for META and TESLA, we need to follow these steps:\n",
      "\n",
      "1. Get the current date.\n",
      "2. Fetch the stock prices for META and TESLA at the beginning of the year and the current stock prices.\n",
      "3. Calculate the YTD gain for both stocks.\n",
      "4. Compare the YTD gains.\n",
      "\n",
      "Let's start by getting the current date and fetching the stock prices. We will use Python for this task, leveraging the `yfinance` library to get the stock data.\n",
      "\n",
      "Here is the Python code to achieve this:\n",
      "\n",
      "```python\n",
      "# filename: ytd_gain_comparison.py\n",
      "\n",
      "import yfinance as yf\n",
      "from datetime import datetime\n",
      "\n",
      "# Get the current date\n",
      "current_date = datetime.now().strftime('%Y-%m-%d')\n",
      "print(f\"Current Date: {current_date}\")\n",
      "\n",
      "# Define the stock tickers\n",
      "stocks = ['META', 'TSLA']\n",
      "\n",
      "# Get the stock data\n",
      "data = yf.download(stocks, start='2023-01-01', end=current_date)\n",
      "\n",
      "# Get the opening prices at the beginning of the year and the latest closing prices\n",
      "start_prices = data['Open'].iloc[0]\n",
      "end_prices = data['Close'].iloc[-1]\n",
      "\n",
      "# Calculate the YTD gain\n",
      "ytd_gain = ((end_prices - start_prices) / start_prices) * 100\n",
      "\n",
      "# Print the YTD gain for each stock\n",
      "for stock in stocks:\n",
      "    print(f\"YTD Gain for {stock}: {ytd_gain[stock]:.2f}%\")\n",
      "```\n",
      "\n",
      "Please save this code in a file named `ytd_gain_comparison.py` and execute it. This script will print the current date and the YTD gain for both META and TESLA.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: Traceback (most recent call last):\n",
      "  File \"/workspaces/AutoPodCaster/src/notebooks/coding/ytd_gain_comparison.py\", line 3, in <module>\n",
      "    import yfinance as yf\n",
      "ModuleNotFoundError: No module named 'yfinance'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "It looks like the `yfinance` library is not installed. Let's first install the `yfinance` library and then run the script again.\n",
      "\n",
      "Here is the updated script that includes the installation of the `yfinance` library:\n",
      "\n",
      "```sh\n",
      "# filename: ytd_gain_comparison.sh\n",
      "\n",
      "# Install yfinance library\n",
      "pip install yfinance\n",
      "\n",
      "# Python script to compare YTD gain for META and TESLA\n",
      "python3 << 'EOF'\n",
      "import yfinance as yf\n",
      "from datetime import datetime\n",
      "\n",
      "# Get the current date\n",
      "current_date = datetime.now().strftime('%Y-%m-%d')\n",
      "print(f\"Current Date: {current_date}\")\n",
      "\n",
      "# Define the stock tickers\n",
      "stocks = ['META', 'TSLA']\n",
      "\n",
      "# Get the stock data\n",
      "data = yf.download(stocks, start='2023-01-01', end=current_date)\n",
      "\n",
      "# Get the opening prices at the beginning of the year and the latest closing prices\n",
      "start_prices = data['Open'].iloc[0]\n",
      "end_prices = data['Close'].iloc[-1]\n",
      "\n",
      "# Calculate the YTD gain\n",
      "ytd_gain = ((end_prices - start_prices) / start_prices) * 100\n",
      "\n",
      "# Print the YTD gain for each stock\n",
      "for stock in stocks:\n",
      "    print(f\"YTD Gain for {stock}: {ytd_gain[stock]:.2f}%\")\n",
      "EOF\n",
      "```\n",
      "\n",
      "Please save this code in a file named `ytd_gain_comparison.sh` and execute it. This script will install the `yfinance` library and then run the Python script to print the current date and the YTD gain for both META and TESLA.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is sh)...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mUserProxyAgent(\n\u001b[1;32m     13\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     human_input_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     },\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# the assistant receives a message from the user_proxy, which contains the task description\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m chat_res \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mWhat date is today? Compare the year-to-date gain for META and TESLA.\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1102\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1104\u001b[0m     summary_method,\n\u001b[1;32m   1105\u001b[0m     summary_args,\n\u001b[1;32m   1106\u001b[0m     recipient,\n\u001b[1;32m   1107\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1108\u001b[0m )\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:904\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:904\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:904\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:738\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient, is_sending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 738\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:902\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2056\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 2056\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   2058\u001b[0m         log_event(\n\u001b[1;32m   2059\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   2065\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1554\u001b[0m, in \u001b[0;36mConversableAgent._generate_code_execution_reply_using_executor\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     iostream\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m   1546\u001b[0m         colored(\n\u001b[1;32m   1547\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>>>>>>> EXECUTING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_code_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CODE BLOCKS (inferred languages are [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([x\u001b[38;5;241m.\u001b[39mlanguage\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mcode_blocks])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1550\u001b[0m         flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1551\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;66;03m# found code blocks, execute code.\u001b[39;00m\n\u001b[0;32m-> 1554\u001b[0m code_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_code_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_code_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1555\u001b[0m exitcode2str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m code_result\u001b[38;5;241m.\u001b[39mexit_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution failed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexitcode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_result\u001b[38;5;241m.\u001b[39mexit_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexitcode2str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCode output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_result\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/coding/local_commandline_code_executor.py:252\u001b[0m, in \u001b[0;36mLocalCommandLineCodeExecutor.execute_code_blocks\u001b[0;34m(self, code_blocks)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_functions_complete:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_functions()\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_code_dont_check_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_blocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/autogen/coding/local_commandline_code_executor.py:311\u001b[0m, in \u001b[0;36mLocalCommandLineCodeExecutor._execute_code_dont_check_setup\u001b[0;34m(self, code_blocks)\u001b[0m\n\u001b[1;32m    308\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m [activation_script, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&&\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mcmd]\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_work_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mTimeoutExpired:\n\u001b[1;32m    315\u001b[0m     logs_all \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m TIMEOUT_MSG\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        # the executor to run the generated code\n",
    "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    },\n",
    ")\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:24:34Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:34Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:34Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of artificial intelligence. Welcome to the podcast, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's episode, we'll be discussing the latest advancements in AI, including new developments in natural language processing, machine learning models, and their applications in various industries.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with natural language processing. Can you tell us about some of the recent breakthroughs in this area?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Absolutely, Pierre. One of the most significant advancements has been the development of transformer-based models like GPT-3. These models have shown remarkable capabilities in understanding and generating human-like text, which has numerous applications from chatbots to content creation.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That's impressive! How about machine learning models? What are some of the new trends we should be aware of?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"insightful\",\n",
      "      \"text\": \"In the realm of machine learning, we're seeing a lot of progress in the area of unsupervised learning and reinforcement learning. These approaches are helping machines learn more efficiently from unlabelled data and improve their decision-making processes in dynamic environments.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"It's amazing to see how far AI has come. Can you give us some examples of how these advancements are being applied in real-world scenarios?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Sure! In healthcare, AI is being used to predict patient outcomes and personalize treatment plans. In finance, it's helping to detect fraudulent transactions and optimize trading strategies. And in everyday consumer applications, AI is enhancing user experiences through personalized recommendations and smart assistants.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing these insights with us. It's been a pleasure having you on the podcast.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre! It was great to be here.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"closing\",\n",
      "      \"text\": \"And that wraps up today's episode of the Advanced AI Podcast. Thank you for tuning in, and we'll see you next time!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:24:41Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:41Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:41Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of AI. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, including new developments in natural language processing, machine learning, and the ethical implications of AI technologies.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with natural language processing. Can you tell us about some of the recent breakthroughs in this area?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Absolutely, Pierre. One of the most significant advancements has been the development of transformer-based models like GPT-3. These models have shown remarkable capabilities in understanding and generating human-like text, which has numerous applications in chatbots, translation services, and content creation.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That's impressive! How about machine learning? What are some of the new trends we should be aware of?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"insightful\",\n",
      "      \"text\": \"In machine learning, we're seeing a lot of progress in areas like reinforcement learning and unsupervised learning. These techniques are helping machines learn more efficiently from their environments and from unlabelled data, which opens up new possibilities for AI applications in robotics, healthcare, and more.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"Interesting. Now, let's talk about the ethical implications of AI. What are some of the key concerns and how are they being addressed?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"serious\",\n",
      "      \"text\": \"Ethical concerns in AI are indeed critical. Issues like bias in AI algorithms, privacy, and the potential for job displacement are major topics of discussion. Researchers and policymakers are working on frameworks and guidelines to ensure that AI technologies are developed and used responsibly, with fairness and transparency being top priorities.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights with us today. It's been a very enlightening discussion.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre! It was a pleasure to be here.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"cheerful\",\n",
      "      \"text\": \"And thank you to our listeners for tuning in to the Advanced AI Podcast. Stay tuned for more episodes where we dive into the world of AI. Goodbye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:24:49Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:49Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:49Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest, Marie, who is an expert in the field of AI. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, including pre-trained autoregressive language models, their applications in various tasks like summarization, machine reading comprehension, and natural language to SQL. We'll also touch on some popular datasets used for training these models.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with pre-trained autoregressive language models. Can you explain what they are and how they work?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Pre-trained autoregressive language models, like GPT, are designed to predict the next word in a sequence based on the context of the previous words. They are trained on large datasets and can be fine-tuned for specific tasks by using smaller, task-specific datasets. This makes them highly versatile for various natural language processing applications.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That's impressive. Can you give us some examples of the tasks these models can be adapted to?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"Absolutely. These models can be adapted for tasks like summarization, where the model generates a concise summary of a given text, machine reading comprehension, where the model answers questions based on a given passage, and natural language to SQL, where the model converts natural language queries into SQL commands. Each of these tasks uses a specific dataset of context-target pairs for training.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"inquisitive\",\n",
      "      \"text\": \"Interesting. You mentioned datasets earlier. Can you tell us more about some of the popular datasets used for training these models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"knowledgeable\",\n",
      "      \"text\": \"Of course. Some popular datasets include the E2E NLG Challenge dataset, which is used for data-to-text generation in the restaurant domain, the DART dataset, which is an open-domain data-to-text dataset with ENTITY — RELATION — ENTITY triples, and the WebNLG dataset, which includes SUBJECT — PROPERTY — OBJECT triples across various categories. These datasets help in training models to generate coherent and contextually accurate text.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights on these advanced AI topics. It was a pleasure having you on the podcast.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre! It was great to be here and discuss these exciting advancements in AI.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"cheerful\",\n",
      "      \"text\": \"And thank you to our listeners for tuning in to the Advanced AI Podcast. Stay tuned for more episodes where we dive into the world of artificial intelligence. Goodbye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:24:58Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:58Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:24:58Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of artificial intelligence. Welcome to the podcast, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's episode, we'll be discussing the latest advancements in AI, including the use of pre-trained autoregressive language models, data-to-text generation tasks, and the significance of datasets like DART, WebNLG, and E2E NLG Challenge.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with pre-trained autoregressive language models. Can you explain what they are and how they are used in AI?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Pre-trained autoregressive language models, such as GPT, are designed to predict the next word in a sequence of words. They are trained on large datasets and can be fine-tuned for specific tasks like summarization, machine reading comprehension, and natural language to SQL conversion. These models have significantly advanced the field of natural language processing.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That's impressive. You mentioned data-to-text generation tasks. Can you tell us more about these tasks and their importance?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"explanatory\",\n",
      "      \"text\": \"Absolutely. Data-to-text generation tasks involve converting structured data into coherent natural language text. This is crucial for applications like automated report generation and conversational agents. Datasets like DART, WebNLG, and E2E NLG Challenge provide the necessary training data for these tasks, helping to improve the quality and accuracy of the generated text.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"I see. Can you give us a brief overview of these datasets and their unique features?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"detailed\",\n",
      "      \"text\": \"Of course. The DART dataset is an open-domain data-to-text dataset with 82,000 examples, making it larger and more complex than others. WebNLG contains 22,000 examples across 14 categories, with a mix of seen and unseen categories during training. The E2E NLG Challenge dataset focuses on the restaurant domain with around 42,000 training examples, and it is known for its multiple reference texts for each input.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights on these advanced AI topics. It has been a pleasure having you on the podcast.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre. It was great to be here and discuss these exciting advancements in AI.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"closing\",\n",
      "      \"text\": \"And that's a wrap for today's episode of the Advanced AI Podcast. Thank you for tuning in, and we'll see you next time!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:25:05Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:05Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:05Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of AI. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, including new research, applications, and the future of AI technology.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with the latest research. Can you tell us about some of the most recent breakthroughs in AI?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Absolutely, Pierre. One of the most exciting recent breakthroughs is in the area of natural language processing, where models like GPT-3 have shown remarkable capabilities in understanding and generating human-like text. Additionally, advancements in reinforcement learning have led to AI systems that can outperform humans in complex games and simulations.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"That's incredible! How are these advancements being applied in real-world scenarios?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"insightful\",\n",
      "      \"text\": \"Great question, Pierre. In the real world, these AI advancements are being used in various industries. For example, in healthcare, AI is helping to diagnose diseases more accurately and quickly. In finance, AI algorithms are improving fraud detection and trading strategies. The possibilities are truly endless.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"It's amazing to see how AI is transforming different sectors. What do you think the future holds for AI technology?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"optimistic\",\n",
      "      \"text\": \"The future of AI is very promising, Pierre. We can expect AI to become even more integrated into our daily lives, making tasks easier and more efficient. There will also be a focus on making AI more ethical and transparent, ensuring that it benefits society as a whole.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights with us today. It's been a pleasure having you on the podcast.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre. It was great to be here.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"cheerful\",\n",
      "      \"text\": \"And thank you to our listeners for tuning in to the Advanced AI Podcast. Stay tuned for more episodes where we explore the fascinating world of AI. Goodbye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:25:11Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:11Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:11Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest, Marie, who is an expert in the field of AI. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, including transformer models, their applications in various tasks like summarization and machine reading comprehension, and the challenges we face in this rapidly evolving field.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with transformer models. Can you explain what they are and why they are so important in the field of AI?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Sure, Pierre. Transformer models, introduced by Vaswani et al. in 2017, are a type of neural network architecture that has revolutionized natural language processing. They use self-attention mechanisms to process input data in parallel, which makes them highly efficient and effective for tasks like language translation, text generation, and more.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"That's very interesting. How are these models being adapted for specific tasks like summarization and machine reading comprehension?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"detailed\",\n",
      "      \"text\": \"Great question, Pierre. For tasks like summarization, we fine-tune pre-trained transformer models on datasets where the input is an article and the output is its summary. Similarly, for machine reading comprehension, the model is trained on context-question-answer pairs, enabling it to understand and generate accurate responses based on the given context.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"inquisitive\",\n",
      "      \"text\": \"What are some of the challenges researchers face when working with these advanced models?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"One of the main challenges is the computational resources required to train these models, as they are quite large and complex. Additionally, ensuring that the models generalize well to unseen data and do not overfit to the training data is another significant challenge. There's also the issue of bias in training data, which can affect the model's performance and fairness.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you for sharing these insights, Marie. It's clear that while there are challenges, the potential of transformer models in AI is immense.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Pierre. It was a pleasure discussing these topics with you.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"closing\",\n",
      "      \"text\": \"Thank you Marie for joining us today. And thank you to our listeners for tuning in to the Advanced AI Podcast. Stay tuned for more episodes where we delve into the fascinating world of AI. Goodbye!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:25:19Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:19Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:19Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of AI. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, including new developments in natural language processing, machine learning, and the impact of AI on various industries.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with natural language processing. Can you tell us about some of the recent breakthroughs in this area?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Absolutely, Pierre. One of the most significant advancements has been the development of transformer-based models like GPT-3. These models have shown remarkable capabilities in understanding and generating human-like text, which has numerous applications in chatbots, translation services, and content creation.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"interested\",\n",
      "      \"text\": \"That's impressive! How about machine learning? What are some of the key trends we should be aware of?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"insightful\",\n",
      "      \"text\": \"In machine learning, we're seeing a lot of progress in areas like reinforcement learning and unsupervised learning. These techniques are helping machines learn more efficiently from their environments and from unlabelled data, which opens up new possibilities for AI applications in robotics, healthcare, and more.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"It's amazing to see how far AI has come. What impact do you think these advancements will have on industries in the near future?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"The impact will be profound, Pierre. AI is set to revolutionize industries by automating routine tasks, providing deeper insights through data analysis, and enhancing decision-making processes. Sectors like finance, healthcare, and manufacturing are already seeing significant benefits, and this trend will only continue to grow.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights with us today. It's been a pleasure having you on the podcast.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre! It was great to be here.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"cheerful\",\n",
      "      \"text\": \"And thank you to our listeners for tuning in to the Advanced AI Podcast. Stay tuned for more episodes where we dive into the world of AI and its incredible advancements. Until next time!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16T13:25:28Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:28Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n",
      "[2024-09-16T13:25:28Z WARN  lance_core::utils::tokio] Number of CPUs is less than or equal to the number of IO core reservations. This is not a supported configuration. using 1 CPU for compute intensive tasks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Advanced AI Podcast\",\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"enthusiastic\",\n",
      "      \"text\": \"Welcome to the Advanced AI Podcast! I'm your host, Pierre, your podcast assistant. Today, we have a special guest with us, Marie, who is an expert in the field of artificial intelligence. Welcome, Marie!\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"warm\",\n",
      "      \"text\": \"Thank you Pierre! I'm excited to be here. In today's podcast, we'll be discussing the latest advancements in AI, the impact of AI on various industries, and some of the ethical considerations surrounding AI development.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"curious\",\n",
      "      \"text\": \"That sounds fascinating, Marie. Let's start with the latest advancements in AI. Can you tell us about some of the most exciting developments in this field?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"informative\",\n",
      "      \"text\": \"Absolutely, Pierre. One of the most exciting advancements is the development of GPT-3, a state-of-the-art language model that can generate human-like text. Additionally, there have been significant improvements in computer vision, allowing AI systems to better understand and interpret visual data.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"engaged\",\n",
      "      \"text\": \"That's incredible! How are these advancements impacting various industries?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"insightful\",\n",
      "      \"text\": \"AI is transforming industries in numerous ways. In healthcare, AI is being used to improve diagnostics and personalize treatment plans. In finance, AI algorithms are enhancing fraud detection and automating trading. The possibilities are truly endless.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"thoughtful\",\n",
      "      \"text\": \"With such powerful technology, there must be ethical considerations to keep in mind. What are some of the key ethical issues surrounding AI development?\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"serious\",\n",
      "      \"text\": \"Indeed, Pierre. One major concern is bias in AI algorithms, which can lead to unfair treatment of certain groups. Another issue is the potential for job displacement due to automation. It's crucial that we address these ethical challenges to ensure AI benefits everyone.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"grateful\",\n",
      "      \"text\": \"Thank you Marie for sharing your insights with us today. It's been a truly enlightening discussion.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Marie\",\n",
      "      \"intonation\": \"appreciative\",\n",
      "      \"text\": \"Thank you Pierre. It was a pleasure to be here.\"\n",
      "    },\n",
      "    {\n",
      "      \"speaker\": \"Pierre\",\n",
      "      \"intonation\": \"closing\",\n",
      "      \"text\": \"And that wraps up today's episode of the Advanced AI Podcast. Thank you for tuning in, and we'll see you next time!\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for segment in range(len(segments_outline)):\n",
    "    # Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "    podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "    {segment}\n",
    "\n",
    "    This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "    The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "    When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "    Output as a JSON with the following fields:\n",
    "    - title: Title of the podcast\n",
    "    - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "    Return only the json as plain text.\n",
    "    \"\"\"\n",
    "\n",
    "    formatted_podcast_prompt = podcast_prompt.format(segment)\n",
    "\n",
    "    podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "    podcast_script_text = podcast_script_response['answer']\n",
    "    print(podcast_script_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "{podcast_outline}\n",
    "\n",
    "This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "The name of the host is Pierre and his role is to be the listener's podcast assistant. The name of the guest is Marie and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Pierre\".\n",
    "\n",
    "Output as a JSON with the following fields:\n",
    "- title: Title of the podcast\n",
    "- text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "Return only the json as plain text.\n",
    "\"\"\"\n",
    "\n",
    "formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast script\n",
    "\n",
    "podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the podcast script\n",
    "\n",
    "podcast_script_file_name = pdf_filename.replace('.pdf', '_script.json')\n",
    "\n",
    "with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "    f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "\n",
    "# Creates an instance of a speech config with specified subscription key and service region.\n",
    "speech_key = os.environ['AZURE_SPEECH_KEY']\n",
    "service_region = os.environ['AZURE_SPEECH_REGION']\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-GuyNeural for Pierre Voice\n",
    "# and en-US-JennyNeural for Marie Voice\n",
    "podcast_script_json = json.loads(str(podcast_script_text))\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Pierre':\n",
    "        ssml_text += f\"<voice name='en-US-GuyNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Marie':\n",
    "        ssml_text += f\"<voice name='en-US-JennyNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "stream = speechsdk.AudioDataStream(result)\n",
    "podcast_filename = pdf_filename.replace('.pdf', '_podcast.wav')\n",
    "stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
