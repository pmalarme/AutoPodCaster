{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU autogen pypdf langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pdf information\n",
    "\n",
    "pdf_title = 'LoRA: Low-Rank Adaptation of Large Language Models'\n",
    "pdf_url = 'https://arxiv.org/pdf/2106.09685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name without special characters\n",
    "\n",
    "pdf_filename = pdf_title.replace(':', '').replace('-', '') + '.pdf'\n",
    "print(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF as langchain document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(get_file(pdf_filename), 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the documents from the PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(get_file(pdf_filename))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata of all documents\n",
    "\n",
    "for document in documents:\n",
    "  document.metadata['title'] = pdf_title\n",
    "  document.metadata['source'] = pdf_url\n",
    "  document.metadata['description'] = ''\n",
    "  document.metadata['thumbnail_url'] = ''\n",
    "  document.metadata['type'] = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the downloaded PDF file\n",
    "\n",
    "os.remove(get_file(pdf_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"/n/n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast on LoRA.\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = pdf_filename.replace('.pdf', '_script.txt')\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "        #print(content)\n",
    "\n",
    "# Example usage\n",
    "file_path = '../../data/How to fine-tune a model using LoRA (step by step)_podcast_outline.txt'  # Replace with the path to your file\n",
    "podcast_outline = read_and_print_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_outline = podcast_outline.split(\"####\")\n",
    "print(segments_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# create an AssistantAgent named \"assistant\"\n",
    "writer = autogen.AssistantAgent(\n",
    "    name=\"writer\",\n",
    "    system_message=\"\"\"You are a writer of a podcast. If you get a bad review from the reviewer on a segment of a podcast, you need to rewrite that podcast segment. \n",
    "    Output the rewritten segment as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "reviewer = autogen.AssistantAgent(\n",
    "    name=\"reviewer\",\n",
    "    system_message=\"\"\"You are a reviewer of a podcast.\n",
    "    If you see questions and answers that are duplicate one, please ask to remove them. \n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    # code_execution_config={\n",
    "    #     # the executor to run the generated code\n",
    "    #     \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    # },\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, reviewer, writer], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_conversation = \"\"\n",
    "full_script = \"\"\n",
    "\n",
    "for segment in range(len(segments_outline[1:])):\n",
    "\n",
    "  \n",
    "    \n",
    "    # Create a prompt with the outline to get a full podcast text\n",
    "    if len(podcast_conversation) == 0:\n",
    "        podcast_prompt = f\"\"\"Create the first segment of a podcast text which is the introduction based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will introduce the podcast and the guest. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic. \n",
    "        The name of the podcast is \"Advanced AI Podcast\".\n",
    "        \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif len(podcast_conversation) != 0 and segment!=len(segments_outline)-1:\n",
    "        podcast_prompt = f\"\"\"Create a segment which is in the middle of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will ask questions about the {segments_outline[segment]} to the guest and the guest will answer them. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic.\n",
    "        This is in the middle of the podcast, so don't welcome the listeners again and don't specify what this segment is about!\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif segment==len(segments_outline)-1 :\n",
    "        podcast_prompt = f\"\"\"Create a segment which is the closing of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest.  \n",
    "        The host will thank the guest and close the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic. \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "\n",
    "    formatted_podcast_prompt = podcast_prompt.format(segments_outline[segment])\n",
    "\n",
    "    podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "    podcast_script_text = podcast_script_response['answer']\n",
    "    \n",
    "    if segment>0:\n",
    "        chat_res = user_proxy.initiate_chat(\n",
    "        recipient=manager,\n",
    "        message=f\"\"\"Combine\"\"\" + podcast_previous_script_text + \" and \" + podcast_script_text + f\"\"\" and evaluate if the conversation is \n",
    "        a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
    "        \"\"\",\n",
    "       \n",
    "        summary_method=\"reflection_with_llm\"\n",
    "        )   \n",
    "    else:\n",
    "        chat_res = user_proxy.initiate_chat(\n",
    "        recipient=manager,\n",
    "        message=f\"\"\"Evaluate if the start of the conversation {podcast_script_text} is \n",
    "        a coherent one, has a good narative. \n",
    "        \"\"\",\n",
    "      \n",
    "        summary_method=\"reflection_with_llm\"\n",
    "        )   \n",
    "\n",
    "    podcast_previous_script_text = podcast_script_text\n",
    "    podcast_conversation = podcast_conversation + podcast_script_text\n",
    "\n",
    "    try:\n",
    "        full_script = full_script + chat_res.chat_history[2]['content']\n",
    "    except:\n",
    "        pass\n",
    "        # print(podcast_script_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(chat_res.chat_history[2]['content'])\n",
    "json_dict = json.loads(\"\"\"{  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
    "  \"text\": [\"\"\" + full_script.replace('TERMINATE','').replace('}{', '},{').replace(\"\"\"\"text\": [\"\"\",'').replace('},{', '},').replace(']','').replace(\"\\n},\\n\",\",\").replace(\"{ \\n \\n  {\", \"[ \\n {\")[1:-1] + \"] }\")\n",
    "print(json_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "# podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "# {podcast_outline}\n",
    "\n",
    "# This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "# The name of the host is Bill and his role is to be the listener's podcast assistant. The name of the guest is Melinda and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "# When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "# Output as a JSON with the following fields:\n",
    "# - title: Title of the podcast\n",
    "# - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "# Return only the json as plain text.\n",
    "# \"\"\"\n",
    "\n",
    "# formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate the podcast script\n",
    "\n",
    "# podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "# podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_string = \"\"\"{  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
    "  \"text\": [\"\"\" + full_script.replace('TERMINATE','').replace('}{', '},{').replace(\"\"\"\"text\": [\"\"\",'').replace('},{', '},').replace(']','').replace(\"\\n},\\n\",\",\").replace(\"{ \\n \\n  {\", \"[ \\n {\")[1:-1] + \"] }\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the podcast script\n",
    "\n",
    "# podcast_script_file_name = pdf_filename.replace('.pdf', '_script.json')\n",
    "\n",
    "# with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "#     f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'How to Fine-Tune a Model Using LoRA (Step by Step)', 'text': [{'speaker': 'Bill', 'intonation': 'enthusiastic', 'text': \"Welcome to the Advanced AI Podcast! I'm your host, Bill, and today we have an exciting episode lined up for you. We're diving deep into the world of fine-tuning models using LoRA, or Low-Rank Adaptation. This is a cutting-edge technique that's making waves in the field of natural language processing.\"}, {'speaker': 'Bill', 'intonation': 'excited', 'text': 'Joining us today is Melinda, an expert in this fascinating topic. Melinda, thank you for being here!'}, {'speaker': 'Melinda', 'intonation': 'warm', 'text': \"Thank you Bill, it's great to be here. I'm really looking forward to discussing how LoRA can make fine-tuning large language models more efficient and accessible.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': \"Fantastic! So, let's get started. Melinda, can you give our listeners a brief overview of what LoRA is and why it's important in the context of fine-tuning large language models?\"}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': \"Sure, Bill. LoRA stands for Low-Rank Adaptation. It's a technique used to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\"}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?'}, {'speaker': 'Melinda', 'intonation': 'enthusiastic', 'text': 'One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.'}, {'speaker': 'Bill', 'intonation': 'thoughtful', 'text': 'I see. And why is it important to fine-tune these large models for specific tasks?'}, {'speaker': 'Melinda', 'intonation': 'explanatory', 'text': 'Fine-tuning allows these models to adapt to specific tasks or domains, improving their performance on those tasks. By leveraging the knowledge from the pre-trained model and adapting it to the specific requirements, we can achieve much better results than using a generic model.'}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, can you explain what LoRA, or Low-Rank Adaptation, is?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': \"Sure, Bill. LoRA stands for Low-Rank Adaptation. It's a technique used to fine-tune large pre-trained models by injecting trainable rank decomposition matrices into each layer of the Transformer architecture.\"}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'That sounds fascinating. What are some of the benefits of using LoRA for fine-tuning models?'}, {'speaker': 'Melinda', 'intonation': 'enthusiastic', 'text': 'One of the main benefits is that it significantly reduces the number of trainable parameters, making the fine-tuning process much more efficient. This is especially important for very large models, where full fine-tuning can be prohibitively expensive.'}, {'speaker': 'Bill', 'intonation': 'thoughtful', 'text': 'I see. And why is it important to fine-tune these large models for specific tasks?'}, {'speaker': 'Melinda', 'intonation': 'explanatory', 'text': 'Fine-tuning allows these models to adapt to specific tasks or domains, improving their performance on those tasks. By leveraging the knowledge from the pre-trained model and adapting it to the specific requirements, we can achieve much better results than using a generic model.'}, {'speaker': 'Bill', 'intonation': 'curious', 'text': \"Melinda, can you explain why it's important to use a GPU for fine-tuning models?\"}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"}, {'speaker': 'Melinda', 'intonation': 'enthusiastic', 'text': \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle most fine-tuning tasks efficiently, yet it's more affordable compared to some of the higher-end GPUs, making it a popular choice for many researchers and developers.\"}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': \"Great, now let's move on to loading the base model. What are the steps involved in this process?\"}, {'speaker': 'Melinda', 'intonation': 'clear', 'text': \"First, you need to select the pre-trained model you want to fine-tune. Once you've chosen the model, you load it into memory using the appropriate library or framework, such as TensorFlow or PyTorch. This usually involves a few lines of code to initialize the model and load its pre-trained weights.\"}, {'speaker': 'Bill', 'intonation': 'inquisitive', 'text': 'And how do you check the number of parameters in the base model?'}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"After loading the model, you can typically print out the number of parameters using a built-in function provided by the framework. For example, in PyTorch, you can use the 'model.parameters()' function to get a list of all the parameters and then sum them up to get the total count.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': \"Melinda, can you explain why it's important to use a GPU for fine-tuning models?\"}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. Using a GPU is crucial for fine-tuning because it significantly speeds up the training process. GPUs are designed to handle the parallel processing required for deep learning tasks, making them much more efficient than CPUs.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': \"That makes sense. I've heard that the L4 GPU is particularly cost-effective. Can you tell us more about that?\"}, {'speaker': 'Melinda', 'intonation': 'enthusiastic', 'text': \"Absolutely. The L4 GPU offers a great balance between performance and cost. It's powerful enough to handle most fine-tuning tasks efficiently, yet it's more affordable compared to some of the higher-end GPUs, making it a popular choice for many researchers and developers.\"}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': \"Great, now let's move on to loading the base model. What are the steps involved in this process?\"}, {'speaker': 'Melinda', 'intonation': 'clear', 'text': \"First, you need to select the pre-trained model you want to fine-tune. Once you've chosen the model, you load it into memory using the appropriate library or framework, such as TensorFlow or PyTorch. This usually involves a few lines of code to initialize the model and load its pre-trained weights.\"}, {'speaker': 'Bill', 'intonation': 'inquisitive', 'text': 'And how do you check the number of parameters in the base model?'}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"After loading the model, you can typically print out the number of parameters using a built-in function provided by the framework. For example, in PyTorch, you can use the 'model.parameters()' function to get a list of all the parameters and then sum them up to get the total count.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, can you explain what rank one matrices are and how they play a role in LoRA?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. Rank one matrices are essentially matrices that can be decomposed into the product of two vectors. In the context of LoRA, they help in reducing the complexity of the weight matrices by approximating them with lower-rank matrices.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'That sounds fascinating. What are the trade-offs between using lower and higher ranks in terms of accuracy?'}, {'speaker': 'Melinda', 'intonation': 'thoughtful', 'text': 'Great question. Lower ranks can significantly reduce the number of parameters, which is beneficial for computational efficiency. However, this might come at the cost of reduced accuracy. On the other hand, higher ranks can maintain or even improve accuracy but will require more computational resources.'}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': 'I see. Moving on to creating the configuration, what are the steps involved in setting up LoRA for fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'clear', 'text': \"The first step is to identify the weight matrices in the self-attention module that you want to adapt. Next, you decompose these matrices into lower-rank matrices. Finally, you set the rank parameter 'r' based on your computational budget and accuracy requirements.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': 'And how important is it to set the right parameters for the task at hand?'}, {'speaker': 'Melinda', 'intonation': 'emphatic', 'text': \"It's crucial, Bill. Setting the right parameters ensures that you strike a balance between computational efficiency and model performance. Incorrect parameters can either lead to underfitting, where the model doesn't learn enough, or overfitting, where it learns too much noise from the training data.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, can you explain what rank one matrices are and how they play a role in LoRA?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. Rank one matrices are essentially matrices that can be decomposed into the product of two vectors. In the context of LoRA, they help in reducing the complexity of the weight matrices by approximating them with lower-rank matrices.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'That sounds fascinating. What are the trade-offs between using lower and higher ranks in terms of accuracy?'}, {'speaker': 'Melinda', 'intonation': 'thoughtful', 'text': 'Great question. Lower ranks can significantly reduce the number of parameters, which is beneficial for computational efficiency. However, this might come at the cost of reduced accuracy. On the other hand, higher ranks can maintain or even improve accuracy but will require more computational resources.'}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': 'I see. Moving on to creating the configuration, what are the steps involved in setting up LoRA for fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'clear', 'text': \"The first step is to identify the weight matrices in the self-attention module that you want to adapt. Next, you decompose these matrices into lower-rank matrices. Finally, you set the rank parameter 'r' based on your computational budget and accuracy requirements.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': 'And how important is it to set the right parameters for the task at hand?'}, {'speaker': 'Melinda', 'intonation': 'emphatic', 'text': \"It's crucial, Bill. Setting the right parameters ensures that you strike a balance between computational efficiency and model performance. Incorrect parameters can either lead to underfitting, where the model doesn't learn enough, or overfitting, where it learns too much noise from the training data.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, can you explain the role of the collate function in data preparation for fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. The collate function is crucial because it helps in batching the data efficiently. It ensures that the data is properly formatted and padded, which is essential for the model to process it correctly during training.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'Why is data preparation so important for effective fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'explaining', 'text': \"Data preparation is vital because it directly impacts the model's performance. Properly prepared data ensures that the model learns the right patterns and generalizes well to new, unseen data. Without it, the model might struggle to understand the task, leading to poor results.\"}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': \"Let's move on to training the model. Can you walk us through the step-by-step process of training using LoRA?\"}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"Absolutely. First, we initialize the model with pre-trained weights. Then, we apply LoRA, which stands for Low-Rank Adaptation, to fine-tune the model. This involves updating only a subset of the model's parameters, making the process more efficient. We monitor the training process closely, adjusting hyperparameters as needed to ensure optimal performance.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': 'How do you monitor the training process and what kind of adjustments might be necessary?'}, {'speaker': 'Melinda', 'intonation': 'clarifying', 'text': 'We monitor the training process by tracking metrics like loss and accuracy on both the training and validation datasets. If the model is overfitting, we might reduce the learning rate or apply regularization techniques. Conversely, if the model is underfitting, we might increase the complexity of the model or provide more training data.'}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, can you explain the role of the collate function in data preparation for fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. The collate function is crucial because it helps in batching the data efficiently. It ensures that the data is properly formatted and padded, which is essential for the model to process it correctly during training.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'Why is data preparation so important for effective fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'explaining', 'text': \"Data preparation is vital because it directly impacts the model's performance. Properly prepared data ensures that the model learns the right patterns and generalizes well to new, unseen data. Without it, the model might struggle to understand the task, leading to poor results.\"}, {'speaker': 'Bill', 'intonation': 'engaged', 'text': \"Let's move on to training the model. Can you walk us through the step-by-step process of training using LoRA?\"}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"Absolutely. First, we initialize the model with pre-trained weights. Then, we apply LoRA, which stands for Low-Rank Adaptation, to fine-tune the model. This involves updating only a subset of the model's parameters, making the process more efficient. We monitor the training process closely, adjusting hyperparameters as needed to ensure optimal performance.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': 'How do you monitor the training process and what kind of adjustments might be necessary?'}, {'speaker': 'Melinda', 'intonation': 'clarifying', 'text': 'We monitor the training process by tracking metrics like loss and accuracy on both the training and validation datasets. If the model is overfitting, we might reduce the learning rate or apply regularization techniques. Conversely, if the model is underfitting, we might increase the complexity of the model or provide more training data.'}, {'speaker': 'Bill', 'intonation': 'curious', 'text': \"Melinda, can you walk us through how you evaluate the model's performance after training?\"}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': \"Sure, Bill. After training, we run a series of evaluations to check the model's performance. This typically involves using a validation dataset to see how well the model predicts outcomes compared to the actual results.\"}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'And how do you interpret the accuracy results from these evaluations?'}, {'speaker': 'Melinda', 'intonation': 'explaining', 'text': \"We look at various metrics like accuracy, precision, recall, and F1 score. These metrics help us understand different aspects of the model's performance, such as how often it makes correct predictions and how well it handles imbalanced data.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': \"That makes sense. Now, let's talk about saving the model. What are the steps to save the fine-tuned model, specifically the LoRA adapter?\"}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"To save the fine-tuned model, we focus on saving the LoRA adapter rather than the entire model. This involves extracting the adapter's parameters and storing them separately. This approach is efficient because it reduces the storage requirements and makes it easier to deploy the model in different environments.\"}, {'speaker': 'Bill', 'intonation': 'clarifying', 'text': 'Why is it important to save only the LoRA adapter and not the entire model?'}, {'speaker': 'Melinda', 'intonation': 'clarifying', 'text': \"Saving only the LoRA adapter is crucial because it allows us to leverage the pre-trained model's capabilities while only storing the additional parameters that were fine-tuned. This not only saves space but also ensures that we can easily update or modify the adapter without needing to retrain the entire model.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': \"Melinda, can you walk us through how you evaluate the model's performance after training?\"}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': \"Sure, Bill. After training, we run a series of evaluations to check the model's performance. This typically involves using a validation dataset to see how well the model predicts outcomes compared to the actual results.\"}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'And how do you interpret the accuracy results from these evaluations?'}, {'speaker': 'Melinda', 'intonation': 'explaining', 'text': \"We look at various metrics like accuracy, precision, recall, and F1 score. These metrics help us understand different aspects of the model's performance, such as how often it makes correct predictions and how well it handles imbalanced data.\"}, {'speaker': 'Bill', 'intonation': 'inquiring', 'text': \"That makes sense. Now, let's talk about saving the model. What are the steps to save the fine-tuned model, specifically the LoRA adapter?\"}, {'speaker': 'Melinda', 'intonation': 'detailed', 'text': \"To save the fine-tuned model, we focus on saving the LoRA adapter rather than the entire model. This involves extracting the adapter's parameters and storing them separately. This approach is efficient because it reduces the storage requirements and makes it easier to deploy the model in different environments.\"}, {'speaker': 'Bill', 'intonation': 'clarifying', 'text': 'Why is it important to save only the LoRA adapter and not the entire model?'}, {'speaker': 'Melinda', 'intonation': 'clarifying', 'text': \"Saving only the LoRA adapter is crucial because it allows us to leverage the pre-trained model's capabilities while only storing the additional parameters that were fine-tuned. This not only saves space but also ensures that we can easily update or modify the adapter without needing to retrain the entire model.\"}, {'speaker': 'Bill', 'intonation': 'curious', 'text': 'Melinda, could you give us a quick summary of the fine-tuning process using LoRA?'}, {'speaker': 'Melinda', 'intonation': 'informative', 'text': 'Sure, Bill. LoRA, or Low-Rank Adaptation, is a method that allows us to fine-tune models by injecting trainable low-rank matrices into each layer of the Transformer architecture. This approach helps in retaining high model quality while significantly reducing the number of trainable parameters.'}, {'speaker': 'Bill', 'intonation': 'interested', 'text': 'That sounds efficient. What are some of the key benefits of using LoRA for model fine-tuning?'}, {'speaker': 'Melinda', 'intonation': 'enthusiastic', 'text': 'One of the main benefits is that it allows for quick task-switching when deployed as a service, as the majority of the model parameters are shared. Additionally, it reduces the computational resources required for fine-tuning, making it both cost-effective and faster compared to traditional methods.'}, {'speaker': 'Bill', 'intonation': 'encouraging', 'text': \"That's impressive. For our listeners who are interested in trying this out, where can they find additional resources and tutorials?\"}, {'speaker': 'Melinda', 'intonation': 'helpful', 'text': 'Listeners can find a wealth of resources and tutorials on the official LoRA website and various machine learning forums. There are also several research papers and GitHub repositories that provide detailed guides and examples.'}, {'speaker': 'Bill', 'intonation': 'grateful', 'text': 'Thank you, Melinda, for sharing your insights. And thank you to our listeners for tuning in. Until next time!'}]}\n"
     ]
    }
   ],
   "source": [
    "podcast_script_text = json_dict\n",
    "print(podcast_script_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def add_ssml_and_style(line, line_style):\n",
    "    # Retrieve environment variables\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    "    azure_endpoint = os.environ['OPENAI_AZURE_ENDPOINT']\n",
    "    api_version = os.environ['OPENAI_API_VERSION']\n",
    "    deployment_name = os.environ['OPENAI_AZURE_DEPLOYMENT'] \n",
    "    \n",
    "        # Initialize AzureOpenAI client\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version\n",
    "    )\n",
    "\n",
    "    if not api_key or not azure_endpoint or not deployment_name:\n",
    "        raise ValueError(\"Environment variables for Azure OpenAI Key and Endpoint are not set.\")\n",
    "\n",
    "    prompt_template = \"\"\"Given following text and its entonation, rewrite the intonations of this text with SSML\n",
    "    Text: {text}\n",
    "    Intonation:\n",
    "    {intonation}\n",
    "    You can use the intonation to add the style to the text as in this example:\n",
    "    '''<mstts:express-as style=\"Excited\" styledegree=\"1\">Hello everyone!</mstts:express-as>'''\n",
    "    The styledegree can go from 0.01 to 2\n",
    "    Note that you do not need to add the \"<speak> and <voice> tags. \n",
    "    Do not change the pitch.\n",
    "    Keep the rate always to medium\n",
    "    ONLY return the imrpoved modified text!!\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(text=line, intonation=line_style)\n",
    "    system_p = \"You are an expert in SSML. You will be given a text and an intonation and you will have to return the same text improved with SSML\"\n",
    "    result = azure_openai_client.chat.completions.create(       \n",
    "        model=deployment_name,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_p},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]).choices[0].message.content\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-AndrewMultilingualNeural for Bill Voice\n",
    "# and en-US-AvaMultilingualNeural for Melinda Voice\n",
    "podcast_script_json = json.loads(str(podcast_string))\n",
    "# podcast_script_json = podcast_script_text\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Bill':\n",
    "        ssml_text += f\"<voice name='en-US-AndrewMultilingualNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Melinda':\n",
    "        ssml_text += f\"<voice name='en-US-AvaMultilingualNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# # use the default speaker as audio output.\n",
    "# speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "# stream = speechsdk.AudioDataStream(result)\n",
    "# podcast_filename = pdf_filename.replace('.pdf', '_podcast.wav')\n",
    "# stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSML content split and saved to multiple files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to split SSML content into chunks\n",
    "def split_ssml(ssml_content, max_length=10000):\n",
    "    chunks = []\n",
    "    while len(ssml_content) > max_length:\n",
    "        split_index = ssml_content.rfind('</voice>', 0, max_length)\n",
    "        if split_index == -1:\n",
    "            raise ValueError(\"Cannot split SSML content properly.\")\n",
    "        chunks.append(ssml_content[:split_index+8])\n",
    "        ssml_content = ssml_content[split_index+8:]\n",
    "    chunks.append(ssml_content)\n",
    "    return chunks\n",
    "\n",
    "# Function to wrap content in SSML tags\n",
    "def wrap_in_ssml(i,content):\n",
    "    #first chunck\n",
    "    if i == 0:\n",
    "        return f\"{content}</speak>\"\n",
    "    #all the subchunks\n",
    "    if i < number_of_chuncks-1:\n",
    "        return f\"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>{content}</speak>\"\n",
    "    # the last chunck\n",
    "    elif i == number_of_chuncks-1:\n",
    "        return f\"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>{content}\"\n",
    "\n",
    "# Split the SSML content into chunks\n",
    "chunks = split_ssml(ssml_text)\n",
    "\n",
    "# lenght of chunks\n",
    "number_of_chuncks = len(chunks)\n",
    "# Save each chunk to a separate SSML file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    wrapped_chunk = wrap_in_ssml(i,chunk)\n",
    "    with open(f'output_part_{i}.ssml', 'w') as file:\n",
    "        file.write(wrapped_chunk)\n",
    "\n",
    "print(\"SSML content split and saved to multiple files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech synthesized to 'LoRA_LowRank_Adaptation_of_Large_Language_Models_podcast_1.wav'\n",
      "Speech synthesized to 'LoRA_LowRank_Adaptation_of_Large_Language_Models_podcast_2.wav'\n",
      "Speech synthesized to 'LoRA_LowRank_Adaptation_of_Large_Language_Models_podcast_3.wav'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "infiles = []\n",
    "\n",
    "# Fetch environment variables\n",
    "speech_key = os.environ.get('AZURE_SPEECH_SERVICE_KEY')\n",
    "service_region = os.environ.get('AZURE_SPEECH_SERVICE_REGION')\n",
    "\n",
    "if not speech_key or not service_region or not service_region_azure:\n",
    "    raise ValueError(\"Environment variables for Azure Speech Key and Region are not set.\")\n",
    "\n",
    "# Function to perform speech synthesis using REST API\n",
    "def synthesize_speech_rest(ssml_content, output_filename):\n",
    "    url = f\"https://{service_region_azure}.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': speech_key,\n",
    "        'Content-Type': 'application/ssml+xml',\n",
    "        'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm',\n",
    "        'User-Agent': 'AutoPodCaster/1.0'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=ssml_content)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(output_filename, 'wb') as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        print(f\"Speech synthesized to '{output_filename}'\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "for i  in range(number_of_chuncks):\n",
    "    file_path = '/workspaces/AutoPodCaster/src/notebooks/output_part_{i}.ssml'\n",
    "    \n",
    "    # Read SSML content\n",
    "    with open(file_path, 'r') as file:\n",
    "        ssml_content = file.read()\n",
    " \n",
    "    # Set the output format to WAV\n",
    "    podcast_filename = pdf_filename.replace(' ', '_').replace('.pdf', f'_podcast_{i+1}.wav')\n",
    "\n",
    "\n",
    "    # Perform speech synthesis using REST API\n",
    "    synthesize_speech_rest(ssml_content, podcast_filename)\n",
    "    \n",
    "    # If code is needed for Azure Speech SDK: this can be used, gave me issues with the region name.\n",
    "    # # Create a synthesizer with the given settings\n",
    "    # speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # # Synthesize the speech\n",
    "    # result = speech_synthesizer.speak_ssml_async(ssml_content).get()\n",
    "\n",
    "    # # Check result\n",
    "    # if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    #     print(\"Speech synthesized to '*.wav'\")\n",
    "    #     stream = speechsdk.AudioDataStream(result)\n",
    "    #     stream.save_to_wav_file(os.path.join(current_dir, podcast_filename))\n",
    "        \n",
    "    # else:\n",
    "    #     print(f\"Speech synthesis canceled: {result.cancellation_details.reason}\")\n",
    "    #     if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "    #         print(f\"Error details: {result.cancellation_details.error_details}\")\n",
    "\n",
    "    # speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "    # result = speech_synthesizer.speak_ssml_async(ssml_content).get()\n",
    "    # stream = speechsdk.AudioDataStream(result)\n",
    "    # podcast_filename = pdf_filename.replace(' ', '_').replace('.pdf', f'_podcast_{i+1}.wav')\n",
    "    # stream.save_to_wav_file(os.path.join(current_dir, podcast_filename))\n",
    "    \n",
    "    \n",
    "    infiles.append(os.path.join(current_dir,podcast_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_of_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwave\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Merge multiple wav files into one\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m outfile \u001b[38;5;241m=\u001b[39m podcast_filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mnumber_of_chunks\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_merged.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m infile \u001b[38;5;129;01min\u001b[39;00m infiles:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'number_of_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "\n",
    "# Merge multiple wav files into one\n",
    "outfile = podcast_filename.replace('_' + str(number_of_chuncks) + '.wav', '_merged.wav')\n",
    "\n",
    "data = []\n",
    "\n",
    "for infile in infiles:\n",
    "    try:\n",
    "        with wave.open(infile, 'rb') as w:\n",
    "            data.append([w.getparams(), w.readframes(w.getnframes())])\n",
    "    except EOFError:\n",
    "        print(f\"Error reading {infile}: Unexpected end of file. Skipping this file.\")\n",
    "    except wave.Error as e:\n",
    "        print(f\"Error reading {infile}: {e}. Skipping this file.\")\n",
    "\n",
    "if data:\n",
    "    with wave.open(outfile, 'wb') as output:\n",
    "        output.setparams(data[0][0])\n",
    "        for params, frames in data:\n",
    "            output.writeframes(frames)\n",
    "    print(f\"Merged podcast saved as {outfile}\")\n",
    "else:\n",
    "    print(\"No valid WAV files to merge.\")\n",
    "\n",
    "# Example usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
