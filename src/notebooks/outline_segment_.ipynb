{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Podcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Create a virtual environment and install the required packages.\n",
    "\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU autogen pypdf langchain langchain-text-splitters langchain-core langchain-community lancedb langchain-openai python-dotenv azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file(file_name: str):\n",
    "  \"\"\"Get file path\n",
    "\n",
    "  Args:\n",
    "      file_name (str): File name\n",
    "\n",
    "  Returns:\n",
    "      File path\n",
    "  \"\"\"\n",
    "  output_folder = 'outputs'\n",
    "  if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "  return os.path.join(output_folder, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pdf information\n",
    "\n",
    "pdf_title = 'LoRA: Low-Rank Adaptation of Large Language Models'\n",
    "pdf_url = 'https://arxiv.org/pdf/2106.09685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name without special characters\n",
    "\n",
    "pdf_filename = pdf_title.replace(':', '').replace('-', '') + '.pdf'\n",
    "print(pdf_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF as langchain document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF file\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(get_file(pdf_filename), 'wb') as file:\n",
    "  file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the documents from the PDF file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(get_file(pdf_filename))\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the metadata of all documents\n",
    "\n",
    "for document in documents:\n",
    "  document.metadata['title'] = pdf_title\n",
    "  document.metadata['source'] = pdf_url\n",
    "  document.metadata['description'] = ''\n",
    "  document.metadata['thumbnail_url'] = ''\n",
    "  document.metadata['type'] = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in chunks of maximum 1000 characters with 200 characters overlap using langchain\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embeddings model\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT_EMBEDDINGS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector store\n",
    "\n",
    "import lancedb\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "db = lancedb.connect(\"/tmp/lancedb\")\n",
    "\n",
    "vectorstore = LanceDB.from_documents(\n",
    "  documents=splits,\n",
    "  embedding=azure_openai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: delete the downloaded PDF file\n",
    "\n",
    "os.remove(get_file(pdf_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the langchain chain to do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt for the chain with embeddings and LLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"/n/n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM model\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],\n",
    "  azure_endpoint=os.environ['OPENAI_AZURE_ENDPOINT'],\n",
    "  api_version=os.environ['OPENAI_API_VERSION'],\n",
    "  azure_deployment=os.environ['OPENAI_AZURE_DEPLOYMENT'],\n",
    "  temperature=0,\n",
    "  top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rag chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the outline of the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_outline_response = rag_chain.invoke({\"input\": \"Create an outline for a podcast on LoRA.\"})\n",
    "podcast_outline = podcast_outline_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the podcast outline\n",
    "\n",
    "podcast_outline_file_name = pdf_filename.replace('.pdf', '_script.txt')\n",
    "\n",
    "with open(get_file(podcast_outline_file_name), \"w\") as f:\n",
    "    f.write(podcast_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the podcast script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "        #print(content)\n",
    "\n",
    "# Example usage\n",
    "file_path = '../../data/How to fine-tune a model using LoRA (step by step)_podcast_outline.txt'  # Replace with the path to your file\n",
    "podcast_outline = read_and_print_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_outline = podcast_outline.split(\"####\")\n",
    "print(segments_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import autogen\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# create an AssistantAgent named \"assistant\"\n",
    "writer = autogen.AssistantAgent(\n",
    "    name=\"writer\",\n",
    "    system_message=\"\"\"You are a writer of a podcast. If you get a bad review from the reviewer on a segment of a podcast, you need to rewrite that podcast segment. \n",
    "    Output the rewritten segment as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "reviewer = autogen.AssistantAgent(\n",
    "    name=\"reviewer\",\n",
    "    system_message=\"\"\"You are a reviewer of a podcast.\n",
    "    If you see questions and answers that are duplicate one, please ask to remove them. \n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    # code_execution_config={\n",
    "    #     # the executor to run the generated code\n",
    "    #     \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "    # },\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, reviewer, writer], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_conversation = \"\"\n",
    "full_script = \"\"\n",
    "\n",
    "for segment in range(len(segments_outline[1:])):\n",
    "\n",
    "  \n",
    "    \n",
    "    # Create a prompt with the outline to get a full podcast text\n",
    "    if len(podcast_conversation) == 0:\n",
    "        podcast_prompt = f\"\"\"Create the first segment of a podcast text which is the introduction based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will introduce the podcast and the guest. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic. \n",
    "        The name of the podcast is \"Advanced AI Podcast\".\n",
    "        \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif len(podcast_conversation) != 0 and segment!=len(segments_outline)-1:\n",
    "        podcast_prompt = f\"\"\"Create a segment which is in the middle of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest. \n",
    "        The host will ask questions about the {segments_outline[segment]} to the guest and the guest will answer them. \n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic.\n",
    "        This is in the middle of the podcast, so don't welcome the listeners again and don't specify what this segment is about!\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "    elif segment==len(segments_outline)-1 :\n",
    "        podcast_prompt = f\"\"\"Create a segment which is the closing of a podcast text based on the following part of an outline:\n",
    "\n",
    "        {segments_outline[segment]}\n",
    "\n",
    "        This text will be used to generate the audio of the podcast. \n",
    "        There are 2 participants in the podcast: the host and the guest.  \n",
    "        The host will thank the guest and close the podcast.\n",
    "        The host should follow the {segments_outline[segment]} for asking questions to the guest.\n",
    "        Both the host and the speaker should stick to the {segments_outline[segment]} as topic of the podcast.\n",
    "        The name of the host is Bill and his role is to be the listener's podcast assistant. \n",
    "        The name of the guest is Melinda and her role is to be the expert in the podcast topic. \n",
    "\n",
    "        When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "        Output as a JSON with the following fields:\n",
    "        - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "        Return only the json as plain text.\n",
    "        \"\"\"\n",
    "\n",
    "    formatted_podcast_prompt = podcast_prompt.format(segments_outline[segment])\n",
    "\n",
    "    podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "    podcast_script_text = podcast_script_response['answer']\n",
    "    \n",
    "    if segment>0:\n",
    "        chat_res = user_proxy.initiate_chat(\n",
    "        recipient=manager,\n",
    "        message=f\"\"\"Combine\"\"\" + podcast_previous_script_text + \" and \" + podcast_script_text + f\"\"\" and evaluate if the conversation is \n",
    "        a coherent one, has a good narative, if it has a same conversation style, and a seamless transition between the two parts. \n",
    "        \"\"\",\n",
    "       \n",
    "        summary_method=\"reflection_with_llm\"\n",
    "        )   \n",
    "    else:\n",
    "        chat_res = user_proxy.initiate_chat(\n",
    "        recipient=manager,\n",
    "        message=f\"\"\"Evaluate if the start of the conversation {podcast_script_text} is \n",
    "        a coherent one, has a good narative. \n",
    "        \"\"\",\n",
    "      \n",
    "        summary_method=\"reflection_with_llm\"\n",
    "        )   \n",
    "\n",
    "    podcast_previous_script_text = podcast_script_text\n",
    "    podcast_conversation = podcast_conversation + podcast_script_text\n",
    "\n",
    "    try:\n",
    "        full_script = full_script + chat_res.chat_history[2]['content']\n",
    "    except:\n",
    "        pass\n",
    "        # print(podcast_script_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(chat_res.chat_history[2]['content'])\n",
    "json_dict = json.loads(\"\"\"{  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
    "  \"text\": [\"\"\" + full_script.replace('TERMINATE','').replace('}{', '},{').replace(\"\"\"\"text\": [\"\"\",'').replace('},{', '},').replace(']','').replace(\"\\n},\\n\",\",\").replace(\"{ \\n \\n  {\", \"[ \\n {\")[1:-1] + \"] }\")\n",
    "print(json_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a prompt with the outline to get a full podcast text\n",
    "\n",
    "# podcast_prompt = f\"\"\"Create a podcast complete text based on the following outline:\n",
    "\n",
    "# {podcast_outline}\n",
    "\n",
    "# This text will be used to generate the audio of the podcast. There are 2 participants in the podcast: the host and the guest. The host will introduce the podcast and the guest. The guest will explain the outline of the podcast. The host will ask questions to the guest and the guest will answer them. The host will thank the guest and close the podcast.\n",
    "# The name of the host is Bill and his role is to be the listener's podcast assistant. The name of the guest is Melinda and her role is to be the expert in the podcast topic. The name of the podcast is \"Advanced AI Podcast\".\n",
    "\n",
    "# When you thanks someone, write \"Thank you\" and the name of the person without a comma. For example, \"Thank you Bill\".\n",
    "\n",
    "# Output as a JSON with the following fields:\n",
    "# - title: Title of the podcast\n",
    "# - text: an array of objects with the speaker, the intonation and the text to be spoken\n",
    "# Return only the json as plain text.\n",
    "# \"\"\"\n",
    "\n",
    "# formatted_podcast_prompt = podcast_prompt.format(podcast_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate the podcast script\n",
    "\n",
    "# podcast_script_response = rag_chain.invoke({\"input\": formatted_podcast_prompt})\n",
    "# podcast_script_text = podcast_script_response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_string = \"\"\"{  \"title\": \"How to Fine-Tune a Model Using LoRA (Step by Step)\",\n",
    "  \"text\": [\"\"\" + full_script.replace('TERMINATE','').replace('}{', '},{').replace(\"\"\"\"text\": [\"\"\",'').replace('},{', '},').replace(']','').replace(\"\\n},\\n\",\",\").replace(\"{ \\n \\n  {\", \"[ \\n {\")[1:-1] + \"] }\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the podcast script\n",
    "\n",
    "# podcast_script_file_name = pdf_filename.replace('.pdf', '_script.json')\n",
    "\n",
    "# with open(get_file(podcast_script_file_name), \"w\") as f:\n",
    "#     f.write(podcast_script_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the podcast audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_script_text = json_dict\n",
    "print(podcast_script_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def add_ssml_and_style(line, line_style):\n",
    "    # Retrieve environment variables\n",
    "    api_key = os.environ['OPENAI_API_KEY']\n",
    "    azure_endpoint = os.environ['OPENAI_AZURE_ENDPOINT']\n",
    "    api_version = os.environ['OPENAI_API_VERSION']\n",
    "    deployment_name = os.environ['OPENAI_AZURE_DEPLOYMENT'] \n",
    "    \n",
    "        # Initialize AzureOpenAI client\n",
    "    azure_openai_client = AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_version=api_version\n",
    "    )\n",
    "\n",
    "    if not api_key or not azure_endpoint or not deployment_name:\n",
    "        raise ValueError(\"Environment variables for Azure OpenAI Key and Endpoint are not set.\")\n",
    "\n",
    "    prompt_template = \"\"\"Given following text and its entonation, rewrite the intonations of this text with SSML\n",
    "    Text: {text}\n",
    "    Intonation:\n",
    "    {intonation}\n",
    "    You can use the intonation to add the style to the text as in this example:\n",
    "    '''<mstts:express-as style=\"Excited\" styledegree=\"1\">Hello everyone!</mstts:express-as>'''\n",
    "    The styledegree can go from 0.01 to 2\n",
    "    Note that you do not need to add the \"<speak> and <voice> tags. \n",
    "    Do not change the pitch.\n",
    "    Keep the rate always to medium\n",
    "    ONLY return the imrpoved modified text!!\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(text=line, intonation=line_style)\n",
    "    system_p = \"You are an expert in SSML. You will be given a text and an intonation and you will have to return the same text improved with SSML\"\n",
    "    result = azure_openai_client.chat.completions.create(       \n",
    "        model=deployment_name,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_p},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]).choices[0].message.content\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# This is an example of SSML (Speech Synthesis Markup Language) format.\n",
    "# <speak version=\"1.0\" xmlns=\"https://www.w3.org/2001/10/synthesis\" xml:lang=\"en-US\">\n",
    "#   <voice name=\"en-US-AvaMultilingualNeural\">\n",
    "#     When you're on the freeway, it's a good idea to use a GPS.\n",
    "#   </voice>\n",
    "# </speak>\n",
    "# Parse the JSON response and create a SSML with en-US-AndrewMultilingualNeural for Bill Voice\n",
    "# and en-US-AvaMultilingualNeural for Melinda Voice\n",
    "podcast_script_json = json.loads(str(podcast_string))\n",
    "# podcast_script_json = podcast_script_text\n",
    "ssml_text = \"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\"\n",
    "for line in podcast_script_json['text']:\n",
    "    speaker = line['speaker']\n",
    "    text = line['text']\n",
    "    if speaker == 'Bill':\n",
    "        ssml_text += f\"<voice name='en-US-AndrewMultilingualNeural'>{text}</voice>\"\n",
    "    elif speaker == 'Melinda':\n",
    "        ssml_text += f\"<voice name='en-US-AvaMultilingualNeural'>{text}</voice>\"\n",
    "ssml_text += \"</speak>\"\n",
    "\n",
    "# # use the default speaker as audio output.\n",
    "# speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# result = speech_synthesizer.speak_ssml_async(ssml_text).get()\n",
    "# stream = speechsdk.AudioDataStream(result)\n",
    "# podcast_filename = pdf_filename.replace('.pdf', '_podcast.wav')\n",
    "# stream.save_to_wav_file(get_file(podcast_filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to split SSML content into chunks\n",
    "def split_ssml(ssml_content, max_length=10000):\n",
    "    chunks = []\n",
    "    while len(ssml_content) > max_length:\n",
    "        split_index = ssml_content.rfind('</voice>', 0, max_length)\n",
    "        if split_index == -1:\n",
    "            raise ValueError(\"Cannot split SSML content properly.\")\n",
    "        chunks.append(ssml_content[:split_index+8])\n",
    "        ssml_content = ssml_content[split_index+8:]\n",
    "    chunks.append(ssml_content)\n",
    "    return chunks\n",
    "\n",
    "# Function to wrap content in SSML tags\n",
    "def wrap_in_ssml(i,content):\n",
    "    #first chunck\n",
    "    if i == 0:\n",
    "        return f\"{content}</speak>\"\n",
    "    #all the subchunks\n",
    "    if i < number_of_chuncks-1:\n",
    "        return f\"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>{content}</speak>\"\n",
    "    # the last chunck\n",
    "    elif i == number_of_chuncks-1:\n",
    "        return f\"<speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='en-US'>{content}\"\n",
    "\n",
    "# Split the SSML content into chunks\n",
    "chunks = split_ssml(ssml_text)\n",
    "\n",
    "# lenght of chunks\n",
    "number_of_chuncks = len(chunks)\n",
    "# Save each chunk to a separate SSML file\n",
    "for i, chunk in enumerate(chunks):\n",
    "    wrapped_chunk = wrap_in_ssml(i,chunk)\n",
    "    with open(f'output_part_{i}.ssml', 'w') as file:\n",
    "        file.write(wrapped_chunk)\n",
    "\n",
    "print(\"SSML content split and saved to multiple files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "infiles = []\n",
    "\n",
    "# Fetch environment variables\n",
    "speech_key = os.environ.get('AZURE_SPEECH_SERVICE_KEY')\n",
    "service_region = os.environ.get('AZURE_SPEECH_SERVICE_REGION')\n",
    "service_region_azure = service_region_azure = service_region.replace(\" \", \"\").lower()\n",
    "\n",
    "if not speech_key or not service_region or not service_region_azure:\n",
    "    raise ValueError(\"Environment variables for Azure Speech Key and Region are not set.\")\n",
    "\n",
    "# Function to perform speech synthesis using REST API\n",
    "def synthesize_speech_rest(ssml_content, output_filename):\n",
    "    url = f\"https://{service_region_azure}.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': speech_key,\n",
    "        'Content-Type': 'application/ssml+xml',\n",
    "        'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm',\n",
    "        'User-Agent': 'AutoPodCaster/1.0'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=ssml_content)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(output_filename, 'wb') as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        print(f\"Speech synthesized to '{output_filename}'\")\n",
    "    else:\n",
    "        print(f\"Speech Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "for i  in range(number_of_chuncks):\n",
    "    file_path = '/workspaces/AutoPodCaster/src/notebooks/output_part_{i}.ssml'\n",
    "    \n",
    "    # Read SSML content\n",
    "    with open(file_path, 'r') as file:\n",
    "        ssml_content = file.read()\n",
    " \n",
    "    # Set the output format to WAV\n",
    "    podcast_filename = pdf_filename.replace(' ', '_').replace('.pdf', f'_podcast_{i+1}.wav')\n",
    "\n",
    "\n",
    "    # Perform speech synthesis using REST API\n",
    "    synthesize_speech_rest(ssml_content, podcast_filename)\n",
    "    \n",
    "    # If code is needed for Azure Speech SDK: this can be used, gave me issues with the region name.\n",
    "    # # Create a synthesizer with the given settings\n",
    "    # speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # # Synthesize the speech\n",
    "    # result = speech_synthesizer.speak_ssml_async(ssml_content).get()\n",
    "\n",
    "    # # Check result\n",
    "    # if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    #     print(\"Speech synthesized to '*.wav'\")\n",
    "    #     stream = speechsdk.AudioDataStream(result)\n",
    "    #     stream.save_to_wav_file(os.path.join(current_dir, podcast_filename))\n",
    "        \n",
    "    # else:\n",
    "    #     print(f\"Speech synthesis canceled: {result.cancellation_details.reason}\")\n",
    "    #     if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "    #         print(f\"Error details: {result.cancellation_details.error_details}\")\n",
    "\n",
    "    # speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "    # result = speech_synthesizer.speak_ssml_async(ssml_content).get()\n",
    "    # stream = speechsdk.AudioDataStream(result)\n",
    "    # podcast_filename = pdf_filename.replace(' ', '_').replace('.pdf', f'_podcast_{i+1}.wav')\n",
    "    # stream.save_to_wav_file(os.path.join(current_dir, podcast_filename))\n",
    "    \n",
    "    \n",
    "    infiles.append(os.path.join('/workspaces/AutoPodCaster/src/notebooks/',podcast_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(podcast_filename)\n",
    "print(infiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "\n",
    "# Merge multiple wav files into one\n",
    "outfile = podcast_filename.replace('_' + str(number_of_chuncks) + '.wav', '_merged.wav')\n",
    "\n",
    "data = []\n",
    "\n",
    "for infile in infiles:\n",
    "    try:\n",
    "        with wave.open(infile, 'rb') as w:\n",
    "            data.append([w.getparams(), w.readframes(w.getnframes())])\n",
    "    except EOFError:\n",
    "        print(f\"Error reading {infile}: Unexpected end of file. Skipping this file.\")\n",
    "    except wave.Error as e:\n",
    "        print(f\"Error reading {infile}: {e}. Skipping this file.\")\n",
    "\n",
    "if data:\n",
    "    with wave.open(outfile, 'wb') as output:\n",
    "        output.setparams(data[0][0])\n",
    "        for params, frames in data:\n",
    "            output.writeframes(frames)\n",
    "    print(f\"Merged podcast saved as {outfile}\")\n",
    "else:\n",
    "    print(\"No valid WAV files to merge.\")\n",
    "\n",
    "# Example usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
